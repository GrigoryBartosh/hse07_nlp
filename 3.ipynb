{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/hse07_nlp/blob/master/3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEFKJ8N3ASVn",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxiSNWcvUHxP",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.CRITICAL)\n",
        "\n",
        "PATH_DATASET_1_TEXTS = os.path.join('data', 'train_sentences.txt')\n",
        "PATH_DATASET_1_TEGS = os.path.join('data', 'train_nes.txt')\n",
        "\n",
        "PATH_DATASET_2 = os.path.join('data', 'collection5.zip')\n",
        "DATASET_2_TOKENS_DICT = {'PER': 'PERSON', 'ORG': 'ORG', 'MEDIA': 'ORG'}\n",
        "\n",
        "PATH_DATASET_TEST = os.path.join('data', 'test.txt')\n",
        "PATH_RESULTS = os.path.join('data', 'results.txt')\n",
        "\n",
        "ID_2_TAG = ['PERSON', 'ORG', 'N']\n",
        "TAG_2_ID = dict((t, i) for i, t in enumerate(ID_2_TAG))\n",
        "\n",
        "MAX_TEXT_LEN = 512\n",
        "\n",
        "EPOCHS_1 = 50\n",
        "EPOCHS_2 = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE_1 = 0.0001\n",
        "LEARNING_RATE_2 = 0.0001\n",
        "W_L2_NORM = 0.0\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRIEQM9JF68E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(PATH_DATASET_1_TEXTS, 'r') as file:\n",
        "    dataset_1_texts = file.readlines()\n",
        "    dataset_1_texts = [' '.join(t.split('\\n')) for t in dataset_1_texts]\n",
        "    dataset_1_texts = [' '.join(t.split()) for t in dataset_1_texts]\n",
        "    \n",
        "with open(PATH_DATASET_1_TEGS, 'r') as file:\n",
        "    dataset_1_tags = file.readlines()\n",
        "    \n",
        "for i in range(len(dataset_1_tags)):\n",
        "    tags = dataset_1_tags[i].split()[:-1]\n",
        "    tags[::3] = list(map(int, tags[::3]))\n",
        "    tags[1::3] = list(map(int, tags[1::3]))\n",
        "    tags = list(zip(tags[::3], tags[1::3], tags[2::3]))\n",
        "    dataset_1_tags[i] = [(t[0], t[0] + t[1], t[2]) for t in tags]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsMjgaCdF68K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile(PATH_DATASET_2) as z:\n",
        "    filenames = z.namelist()[1:]\n",
        "    filenames = set([f[:-4] for f in filenames])\n",
        "\n",
        "    d = DATASET_2_TOKENS_DICT\n",
        "    dataset_2_texts, dataset_2_tags = [], []\n",
        "    for filename in filenames:\n",
        "        with z.open(filename + '.txt', 'r') as file:\n",
        "            text = file.read().decode('utf8')\n",
        "        text = ' '.join(text.split('\\r'))\n",
        "\n",
        "        with z.open(filename + '.ann', 'r') as file:\n",
        "            tags = file.readlines()\n",
        "            tags = [l.decode('utf8') for l in tags]\n",
        "            \n",
        "        tags = [t.split('\\t')[1].split() for t in tags]\n",
        "        tags = [(int(t[1]), int(t[2]), t[0]) for t in tags]\n",
        "        tags = [(t[0], t[1], t[2]) for t in tags]\n",
        "        tags = [(t[0], t[1], d[t[2]]) for t in tags if t[2] in d]\n",
        "\n",
        "        dataset_2_texts.append(text)\n",
        "        dataset_2_tags.append(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XsBMrKeF68O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    do_lower_case=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRwIejl9F68S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sample(text, tags, do_split=False):\n",
        "    last_i = 0\n",
        "    data = []\n",
        "    for s, f, tag in tags:\n",
        "        if last_i != s and text[last_i:s] != ' ':\n",
        "            data.append((text[last_i:s], 'N'))\n",
        "            \n",
        "        data.append((text[s:f], tag))\n",
        "        last_i = f\n",
        "        \n",
        "    if last_i != len(text):\n",
        "        data.append((text[last_i:], 'N'))\n",
        "    \n",
        "    tokens, labels = [], []\n",
        "    for s, t in data:\n",
        "        new_tokens = tokenizer.tokenize(s)\n",
        "        labels += [t] * len(new_tokens)\n",
        "        tokens += new_tokens\n",
        "    \n",
        "    length = np.random.randint(30, 130)    \n",
        "    if do_split and len(tokens) > length:\n",
        "        part_length = length // 3\n",
        "        stride = 3 * part_length\n",
        "        nrow = np.ceil(len(tokens) / part_length) - 2\n",
        "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
        "        indexes = indexes.astype(np.int32)\n",
        "\n",
        "        max_indexe = indexes.max()\n",
        "        diff = max_indexe + 1 - len(tokens)\n",
        "        tokens += int(diff > 0) * [tokenizer.sep_token] + \\\n",
        "                  max(0, diff - 1) * [tokenizer.pad_token]\n",
        "        labels += diff * ['N']\n",
        "\n",
        "        tokens = np.array(tokens)[indexes].tolist()\n",
        "        labels = np.array(labels)[indexes].tolist()\n",
        "    else:\n",
        "        tokens = [tokens]\n",
        "        labels = [labels]\n",
        "        \n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = [tokenizer.cls_token] + tokens[i]\n",
        "        labels[i] = ['N'] + labels[i]\n",
        "        \n",
        "        if tokens[i][-1] not in [tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            tokens[i] = tokens[i] + [tokenizer.sep_token]\n",
        "            labels[i] = labels[i] + ['N']\n",
        "\n",
        "    return tokens, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQOrYKNIF68W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_tokens, dataset_labels = [], []\n",
        "\n",
        "for text, tags in zip(dataset_1_texts, dataset_1_tags):\n",
        "    tokens, labels = prepare_sample(text, tags, do_split=False)\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels\n",
        "    \n",
        "for text, tags in zip(dataset_2_texts, dataset_2_tags):\n",
        "    tokens, labels = prepare_sample(text, tags, do_split=True)\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels\n",
        "    \n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    dataset_tokens, dataset_labels, test_size=0.1)\n",
        "train_data = list(zip(x_train, y_train))\n",
        "val_data = list(zip(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cNaV7dtKyYVk",
        "colab": {}
      },
      "source": [
        "def text_collate_fn(texts):\n",
        "    max_len = max([len(text) for text in texts])\n",
        "    lens = [text.index(tokenizer.sep_token) + 1 for text in texts]\n",
        "    masks = [[1] * l + [0] * (max_len - l) for text, l in zip(texts, lens)]\n",
        "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
        "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
        "    texts = torch.LongTensor(texts)\n",
        "    masks = torch.LongTensor(masks)\n",
        "\n",
        "    return texts, masks\n",
        "\n",
        "def collate_fn(data):\n",
        "    texts, labels = zip(*data)\n",
        "\n",
        "    texts, masks = text_collate_fn(texts)\n",
        "\n",
        "    max_len = max([len(l) for l in labels])\n",
        "    labels = [l + ['N'] * (max_len - len(l)) for l in labels]\n",
        "    labels = [[TAG_2_ID[l] for l in tags] for tags in labels]\n",
        "    labels = torch.LongTensor(labels)\n",
        "    \n",
        "    return texts, masks, labels\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_data_loader = data.DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX5tyt31ASV4",
        "colab": {}
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(16, 3)\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, text, mask):\n",
        "        x = self.bert(text, attention_mask=mask)[0]\n",
        "        x = self.layers(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZkLBuIWcuuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedCrossEntropyLoss(nn.Module):\n",
        "    EPS  = 1e-8\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MaskedCrossEntropyLoss, self).__init__()\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, output, target, mask):\n",
        "        x = self.softmax(output)\n",
        "        x = -torch.log(x + MaskedCrossEntropyLoss.EPS)\n",
        "        x = torch.gather(x, 1, target[:, None, :])\n",
        "        x = (x * mask).mean()\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9flBRMybom7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class F1:\n",
        "    def __init__(self):\n",
        "        self.s = np.zeros((3, 3))\n",
        "\n",
        "    def update(self, output, target, mask):\n",
        "        output = output.argmax(axis=1)\n",
        "        output = output.reshape(-1)\n",
        "        target = target.reshape(-1)\n",
        "        mask = mask.reshape(-1)\n",
        "        indexes = np.stack((output, target), axis=1)\n",
        "        indexes = indexes[mask > 0]\n",
        "        self.s[indexes] = self.s[indexes] + 1\n",
        "\n",
        "    def _get(self, id):\n",
        "        f1_person = self.s[id, id] / self.s[id, :].sum()\n",
        "        f1_recall = self.s[id, id] / self.s[:, id].sum()\n",
        "        return 2 * f1_person * f1_recall / (f1_person + f1_recall)\n",
        "\n",
        "    def get(self):\n",
        "        f1_person = self._get(TAG_2_ID['PERSON'])\n",
        "        f1_org = self._get(TAG_2_ID['ORG'])\n",
        "        return (f1_person + f1_org) / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qV3ME8x9ASV7",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, epochs):\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "    f1_train = []\n",
        "    f1_val = []\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        losses = []\n",
        "        f1 = F1()\n",
        "        model.train()\n",
        "        for texts, masks, labels in train_data_loader:\n",
        "            texts = texts.to(device)\n",
        "            masks = masks.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ps = model(texts, masks)\n",
        "            loss = criterion(ps, labels, masks)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            f1.update(\n",
        "                ps.cpu().detach().numpy(),\n",
        "                labels.cpu().detach().numpy(),\n",
        "                masks.cpu().detach().numpy()\n",
        "            )\n",
        "\n",
        "        losses_train.append(np.array(losses).mean())\n",
        "        f1_train.append(f1.get())\n",
        "\n",
        "        losses = []\n",
        "        f1 = F1()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for texts, masks, labels in val_data_loader:\n",
        "                texts = texts.to(device)\n",
        "                masks = masks.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                ps = model(texts, masks)\n",
        "                loss = criterion(ps, labels, masks)\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                f1.update(\n",
        "                    ps.cpu().detach().numpy(),\n",
        "                    labels.cpu().detach().numpy(),\n",
        "                    masks.cpu().detach().numpy()\n",
        "                )\n",
        "\n",
        "        losses_val.append(np.array(losses).mean())\n",
        "        f1_val.append(f1.get())\n",
        "\n",
        "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
        "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(epochs), f1_train, label=\"train\")\n",
        "    plt.plot(range(epochs), f1_val, label=\"val\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('f1')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhrRW2mASV_",
        "colab": {}
      },
      "source": [
        "model = TextClassifier()\n",
        "model.to(device)\n",
        "\n",
        "criterion = MaskedCrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_1,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "#train(model, criterion, optimizer, EPOCHS_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqDJmqPB8zjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, criterion, optimizer, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h2y5i6l3eR0i",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_2,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "#train(model, criterion, optimizer, EPOCHS_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dgkUIU5J6Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, criterion, optimizer, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzyQj0SprR3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_test(text):\n",
        "    parts, parts_poses = [], []\n",
        "    part, part_start = '', 0\n",
        "    for i, c in enumerate(text):\n",
        "        if c.isalpha() or (c.isdigit() and part != '' and part[-1].isdigit()):\n",
        "            part += c\n",
        "        else:\n",
        "            if len(part) > 0:\n",
        "                parts.append(part)\n",
        "                parts_poses.append((part_start, i))\n",
        "\n",
        "            if c.isdigit():\n",
        "                part, part_start = c, i\n",
        "            else:\n",
        "                if c != ' ':\n",
        "                    parts.append(c)\n",
        "                    parts_poses.append((i, i + 1))\n",
        "\n",
        "                part, part_start = '', i + 1\n",
        "\n",
        "    parts.append(part)\n",
        "    parts_poses.append((part_start, len(text)))\n",
        "\n",
        "    tokens, tokens_poses = [], []\n",
        "    for part, poses in zip(parts, parts_poses):\n",
        "        new_tokens = tokenizer.tokenize(part)\n",
        "        tokens_poses += [poses for _ in new_tokens]\n",
        "        tokens += new_tokens\n",
        "\n",
        "    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "    tokens_poses = [(-1, -1)] + tokens_poses + [(-1, -1)]\n",
        "\n",
        "    return tokens, tokens_poses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oQI8sgvyBeDr",
        "colab": {}
      },
      "source": [
        "with open(PATH_DATASET_TEST, 'r') as file:\n",
        "    dataset = file.readlines()\n",
        "    dataset = [' '.join(t.split('\\n')) for t in dataset]\n",
        "    dataset = [' '.join(t.split()) for t in dataset]\n",
        "\n",
        "res = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    while len(dataset) > 0:\n",
        "        texts, dataset = dataset[:BATCH_SIZE], dataset[BATCH_SIZE:]\n",
        "        texts, tokens_poses = zip(*[prepare_test(text) for text in texts])\n",
        "        texts, masks = text_collate_fn(texts)\n",
        "        texts = texts.to(device)\n",
        "        masks = texts.to(device)\n",
        "\n",
        "        ps = model(texts, masks)\n",
        "        ps = ps.cpu().numpy()\n",
        "        labels = ps.argmax(axis=1)\n",
        "        \n",
        "        for s_labels, s_poses in zip(labels, tokens_poses):\n",
        "            d = dict([(p, [0] * 3) for p in s_poses])\n",
        "            for l, p in zip(s_labels, s_poses):\n",
        "                d[p][l] += 1\n",
        "\n",
        "            loc_res = []\n",
        "            for p, stat in d.items():\n",
        "                if p != (-1, -1) and ID_2_TAG[np.argmax(stat)] != 'N':\n",
        "                    loc_res.append(f'{p[0]} {p[1] - p[0]} {ID_2_TAG[np.argmax(stat)]}')\n",
        "            \n",
        "            loc_res.append('EOL')\n",
        "            res.append(' '.join(loc_res))\n",
        "\n",
        "res = '\\n'.join(res)\n",
        "with open(PATH_RESULTS, 'w') as file:\n",
        "    file.write(res)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}