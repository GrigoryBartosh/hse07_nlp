{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/hse07_nlp/blob/master/3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEFKJ8N3ASVn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "ed321ea3-0834-4109-dcdb-2752e7d2d7cf"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxiSNWcvUHxP",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.CRITICAL)\n",
        "\n",
        "PATH_DATASET_1_TEXTS = os.path.join('data', 'train_sentences.txt')\n",
        "PATH_DATASET_1_TEGS = os.path.join('data', 'train_nes.txt')\n",
        "\n",
        "PATH_DATASET_2 = os.path.join('data', 'collection5.zip')\n",
        "DATASET_2_TOKENS_DICT = {'PER': 'PERSON', 'ORG': 'ORG', 'MEDIA': 'ORG'}\n",
        "\n",
        "PATH_DATASET_TEST = os.path.join('data', 'test.txt')\n",
        "PATH_RESULTS = os.path.join('data', 'results.txt')\n",
        "\n",
        "ID_2_TAG = ['PERSON', 'ORG', 'N']\n",
        "TAG_2_ID = dict((t, i) for i, t in enumerate(ID_2_TAG))\n",
        "\n",
        "MAX_TEXT_LEN = 512\n",
        "\n",
        "EPOCHS_1 = 10\n",
        "EPOCHS_2 = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE_1 = 0.0001\n",
        "LEARNING_RATE_2 = 0.00003\n",
        "W_L2_NORM = 0.0\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRIEQM9JF68E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(PATH_DATASET_1_TEXTS, 'r') as file:\n",
        "    dataset_1_texts = file.readlines()\n",
        "    dataset_1_texts = [' '.join(t.split('\\n')) for t in dataset_1_texts]\n",
        "    dataset_1_texts = [' '.join(t.split()) for t in dataset_1_texts]\n",
        "    \n",
        "with open(PATH_DATASET_1_TEGS, 'r') as file:\n",
        "    dataset_1_tags = file.readlines()\n",
        "    \n",
        "for i in range(len(dataset_1_tags)):\n",
        "    tags = dataset_1_tags[i].split()[:-1]\n",
        "    tags[::3] = list(map(int, tags[::3]))\n",
        "    tags[1::3] = list(map(int, tags[1::3]))\n",
        "    tags = list(zip(tags[::3], tags[1::3], tags[2::3]))\n",
        "    dataset_1_tags[i] = [(t[0], t[0] + t[1], t[2]) for t in tags]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1TTAyNc0psKa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsMjgaCdF68K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile(PATH_DATASET_2) as z:\n",
        "    filenames = z.namelist()[1:]\n",
        "    filenames = set([f[:-4] for f in filenames])\n",
        "\n",
        "    d = DATASET_2_TOKENS_DICT\n",
        "    dataset_2_texts, dataset_2_tags = [], []\n",
        "    for filename in filenames:\n",
        "        with z.open(filename + '.txt', 'r') as file:\n",
        "            text = file.read().decode('utf8')\n",
        "        text = ' '.join(text.split('\\r'))\n",
        "\n",
        "        with z.open(filename + '.ann', 'r') as file:\n",
        "            tags = file.readlines()\n",
        "            tags = [l.decode('utf8') for l in tags]\n",
        "            \n",
        "        tags = [t.split('\\t')[1].split() for t in tags]\n",
        "        tags = [(int(t[1]), int(t[2]), t[0]) for t in tags]\n",
        "        tags = [(t[0], t[1], t[2]) for t in tags]\n",
        "        tags = [(t[0], t[1], d[t[2]]) for t in tags if t[2] in d]\n",
        "\n",
        "        dataset_2_texts.append(text)\n",
        "        dataset_2_tags.append(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XsBMrKeF68O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    do_lower_case=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRwIejl9F68S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sample(text, tags, do_split=False):\n",
        "    last_i = 0\n",
        "    data = []\n",
        "    for s, f, tag in tags:\n",
        "        if last_i != s and text[last_i:s] != ' ':\n",
        "            data.append((text[last_i:s], 'N'))\n",
        "            \n",
        "        data.append((text[s:f], tag))\n",
        "        last_i = f\n",
        "        \n",
        "    if last_i != len(text):\n",
        "        data.append((text[last_i:], 'N'))\n",
        "    \n",
        "    tokens, labels = [], []\n",
        "    for s, t in data:\n",
        "        new_tokens = tokenizer.tokenize(s)\n",
        "        labels += [t] * len(new_tokens)\n",
        "        tokens += new_tokens\n",
        "    \n",
        "    length = np.random.randint(30, 130)    \n",
        "    if do_split and len(tokens) > length:\n",
        "        part_length = length // 3\n",
        "        stride = 3 * part_length\n",
        "        nrow = np.ceil(len(tokens) / part_length) - 2\n",
        "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
        "        indexes = indexes.astype(np.int32)\n",
        "\n",
        "        max_indexe = indexes.max()\n",
        "        diff = max_indexe + 1 - len(tokens)\n",
        "        tokens += int(diff > 0) * [tokenizer.sep_token] + \\\n",
        "                  max(0, diff - 1) * [tokenizer.pad_token]\n",
        "        labels += diff * ['N']\n",
        "\n",
        "        tokens = np.array(tokens)[indexes].tolist()\n",
        "        labels = np.array(labels)[indexes].tolist()\n",
        "    else:\n",
        "        tokens = [tokens]\n",
        "        labels = [labels]\n",
        "        \n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = [tokenizer.cls_token] + tokens[i]\n",
        "        labels[i] = ['N'] + labels[i]\n",
        "        \n",
        "        if tokens[i][-1] not in [tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            tokens[i] = tokens[i] + [tokenizer.sep_token]\n",
        "            labels[i] = labels[i] + ['N']\n",
        "\n",
        "    return tokens, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQOrYKNIF68W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_tokens, dataset_labels = [], []\n",
        "\n",
        "for text, tags in zip(dataset_1_texts, dataset_1_tags):\n",
        "    tokens, labels = prepare_sample(text, tags, do_split=False)\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels\n",
        "    \n",
        "for text, tags in zip(dataset_2_texts, dataset_2_tags):\n",
        "    tokens, labels = prepare_sample(text, tags, do_split=True)\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels\n",
        "    \n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    dataset_tokens, dataset_labels, test_size=0.1)\n",
        "train_data = list(zip(x_train, y_train))\n",
        "val_data = list(zip(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cNaV7dtKyYVk",
        "colab": {}
      },
      "source": [
        "def text_collate_fn(texts):\n",
        "    max_len = max([len(text) for text in texts])\n",
        "    lens = [text.index(tokenizer.sep_token) + 1 for text in texts]\n",
        "    masks = [[1] * l + [0] * (max_len - l) for text, l in zip(texts, lens)]\n",
        "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
        "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
        "    texts = torch.LongTensor(texts)\n",
        "    masks = torch.LongTensor(masks)\n",
        "\n",
        "    return texts, masks\n",
        "\n",
        "def collate_fn(data):\n",
        "    texts, labels = zip(*data)\n",
        "\n",
        "    texts, masks = text_collate_fn(texts)\n",
        "\n",
        "    max_len = max([len(l) for l in labels])\n",
        "    labels = [l + ['N'] * (max_len - len(l)) for l in labels]\n",
        "    labels = [[TAG_2_ID[l] for l in tags] for tags in labels]\n",
        "    labels = torch.LongTensor(labels)\n",
        "    \n",
        "    return texts, masks, labels\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_data_loader = data.DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX5tyt31ASV4",
        "colab": {}
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(16, 3)\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, text, mask):\n",
        "        x = self.bert(text, attention_mask=mask)[0]\n",
        "        x = self.layers(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qV3ME8x9ASV7",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, epochs):\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        losses = []\n",
        "        model.train()\n",
        "        for texts, masks, labels in train_data_loader:\n",
        "            texts = texts.to(device)\n",
        "            masks = masks.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ps = model(texts, masks)\n",
        "            loss = criterion(ps, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        losses_train.append(np.array(losses).mean())\n",
        "\n",
        "        losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for texts, masks, labels in val_data_loader:\n",
        "                texts = texts.to(device)\n",
        "                masks = masks.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                ps = model(texts, masks)\n",
        "                loss = criterion(ps, labels)\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        losses_val.append(np.array(losses).mean())\n",
        "\n",
        "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
        "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhrRW2mASV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "3ace9b2d-445f-4e72-db61-41c6b2cb6db6"
      },
      "source": [
        "model = TextClassifier()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_1,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "train(model, criterion, optimizer, EPOCHS_1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [16:21<00:00, 98.16s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3hc9X3n8fd3RiONdZct+a6xDRiM\nzcVSHIcUkuZCE0iymN2EAAkpadKyeTY0t+1uySabdNnkWdp0u+mFlpCENDQQL4GkYVMIbQgkpUCC\nsR1sgw22wbaMZcs33W8jffePcySN5LEt2Rqd0ejzep555lx+Z/SVwProd37nd465OyIiImPFoi5A\nRETykwJCRESyUkCIiEhWCggREclKASEiIlkVRV3AZKmtrfWlS5dGXYaIyLTy/PPPH3b3umz7CiYg\nli5dyoYNG6IuQ0RkWjGzPSfbp1NMIiKSlQJCRESyymlAmNlVZrbDzHaa2W1Z9n/CzLaY2WYze8rM\nVobbl5pZd7h9s5ndlcs6RUTkRDkbgzCzOHAn8DtAE/CcmT3s7i9mNLvf3e8K218D/AVwVbhvl7uv\nzlV9IiIA/f39NDU10dPTE3UpOZVMJlm8eDGJRGLcx+RykHotsNPddwOY2XpgHTAcEO7eltG+DNCN\noURkSjU1NVFRUcHSpUsxs6jLyQl358iRIzQ1NbFs2bJxH5fLU0yLgH0Z603htlHM7JNmtgv4M+BT\nGbuWmdkmM/uFmb0l2xcws1vMbIOZbWhpaZnM2kVkhujp6WHOnDkFGw4AZsacOXMm3EuKfJDa3e90\n93OBPwa+GG4+AKTcvQH4HHC/mVVmOfZud1/j7mvq6rJexisiclqFHA5DzuR7zGVA7AfqM9YXh9tO\nZj1wLYC797r7kXD5eWAXcH4uijze1cdf/uwVtu5vzcXHi4hMW7kMiOeA5Wa2zMyKgRuAhzMbmNny\njNX3Aq+E2+vCQW7M7BxgObA7F0XGY8bXH3+Zx186lIuPFxE5pePHj/O3f/u3Ez7uPe95D8ePH89B\nRSNyFhDungZuBR4DXgIecPdtZnZ7eMUSwK1mts3MNhOcSro53P5W4IVw+4PAJ9z9aC7qrEgmOH9u\nBZv2HcvFx4uInNLJAiKdTp/yuEceeYTq6upclQXk+FYb7v4I8MiYbV/KWP70SY57CHgol7VlakhV\n8+jWZgYHnVis8M9Fikj+uO2229i1axerV68mkUiQTCapqalh+/btvPzyy1x77bXs27ePnp4ePv3p\nT3PLLbcAI7cX6ujo4Oqrr+aKK67g6aefZtGiRfz4xz9m1qxZZ11bwdyL6Ww0pmpY/9w+dh/u5Ly5\n5VGXIyIR+R//bxsvvt52+oYTsHJhJV/+d6tOuv+OO+5g69atbN68mSeffJL3vve9bN26dfhy1Hvu\nuYfZs2fT3d3NG9/4Rt7//vczZ86cUZ/xyiuv8P3vf59vfvObfPCDH+Shhx7ipptuOuvaI7+KKR80\npIJu2qa9Os0kItFau3btqLkKf/VXf8Wll17KZZddxr59+3jllVdOOGbZsmWsXh3MK37DG97Aa6+9\nNim1qAcBnFtXTkWyiI17j3PdmvrTHyAiBelUf+lPlbKysuHlJ598kp/97Gc888wzlJaW8ra3vS3r\nXIaSkpLh5Xg8Tnd396TUoh4EEIsZq+ur1YMQkSlXUVFBe3t71n2tra3U1NRQWlrK9u3befbZZ6e0\nNvUgQo2pGv7656/Q0ZumvEQ/FhGZGnPmzOHyyy/noosuYtasWcybN29431VXXcVdd93FhRdeyAUX\nXMBll102pbXpN2GoIVXNoMML+47zW+fVRl2OiMwg999/f9btJSUlPProo1n3DY0z1NbWsnXr1uHt\nf/RHfzRpdekUU6ihvgaAjTrNJCICKCCGVZUmOG9uOZv25nZmoojIdKGAyNBQX82mfcdx113HRUQU\nEBkal9RwtLOPPUe6oi5FRCRyCogMQxPmNA4hIqKAGGX53ArKS4oUECIiKCBGiceMS+urNFAtInmr\nvHzq7hengBijMVXD9uZ2uvpOfatdEZFCp4lyYzSkqhkYdF5oauWyc+ac/gARkbNw2223UV9fzyc/\n+UkA/uRP/oSioiKeeOIJjh07Rn9/P1/5yldYt27dlNemgBgjc8KcAkJkhnn0NmjeMrmfOf9iuPqO\nk+6+/vrr+cxnPjMcEA888ACPPfYYn/rUp6isrOTw4cNcdtllXHPNNVP+7GwFxBg1ZcUsqy3TOISI\nTImGhgYOHTrE66+/TktLCzU1NcyfP5/Pfvaz/PKXvyQWi7F//34OHjzI/Pnzp7Q2BUQWDalqfvly\nC+4+5YktIhE6xV/6uXTdddfx4IMP0tzczPXXX899991HS0sLzz//PIlEgqVLl2a9zXeuaZA6i8ZU\nDYc7+mg6Njn3VBcROZXrr7+e9evX8+CDD3LdddfR2trK3LlzSSQSPPHEE+zZsyeSuhQQWWjCnIhM\npVWrVtHe3s6iRYtYsGABH/7wh9mwYQMXX3wx9957LytWrIikLp1iyuKCeRWUFsfZtPc461Yviroc\nEZkBtmwZGRyvra3lmWeeydquo6NjqkpSDyKboniMSxZXqQchIjOaAuIkGlM1vPh6Gz39A1GXIiIS\niZwGhJldZWY7zGynmd2WZf8nzGyLmW02s6fMbGXGvs+Hx+0ws3fnss5sGlI1pAedLftbp/pLi8gU\nmwm3+D+T7zFnAWFmceBO4GpgJXBjZgCE7nf3i919NfBnwF+Ex64EbgBWAVcBfxt+3pQZHqjeo9NM\nIoUsmUxy5MiRgg4Jd+fIkSMkk8kJHZfLQeq1wE533w1gZuuBdcCLQw3cvS2jfRkw9F9oHbDe3XuB\nV81sZ/h52UdtcqC2vITU7FJNmBMpcIsXL6apqYmWlpaoS8mpZDLJ4sWLJ3RMLgNiEbAvY70JeNPY\nRmb2SeBzQDHwjoxjnx1z7AmXE5nZLcAtAKlUalKKztSYqubpXUc0YU6kgCUSCZYtWxZ1GXkp8kFq\nd7/T3c8F/hj44gSPvdvd17j7mrq6ukmvrSFVw6H2Xl5vnfoZjCIiUctlQOwH6jPWF4fbTmY9cO0Z\nHpsTjanwxn0ahxCRGSiXAfEcsNzMlplZMcGg88OZDcxsecbqe4FXwuWHgRvMrMTMlgHLgV/nsNas\nViyoIJmIaRxCRGaknI1BuHvazG4FHgPiwD3uvs3Mbgc2uPvDwK1mdiXQDxwDbg6P3WZmDxAMaKeB\nT7r7lE9ISMRjXLKoWhPmRGRGyumtNtz9EeCRMdu+lLH86VMc+1Xgq7mrbnwallTznadeozc9QEnR\nlF5pKyISqcgHqfNdQ30NfQODbN3fdvrGIiIFRAFxGo3hhLlNOs0kIjOMAuI05lYmWVQ9SwPVIjLj\nKCDGoXFJjQaqRWTGUUCMQ0N9NQdaezjQqifMicjMoYAYh8YlwYQ5nWYSkZlEATEOKxdUUlwU00C1\niMwoCohxKC6KcfGiKjaqByEiM4gCYpwaU9Vs2d9KX3ow6lJERKaEAmKcGlI19KUHefGAJsyJyMyg\ngBinoTu7ahxCRGYKBcQ4za9KsqAqqXEIEZkxFBAT0Jiq0bMhRGTGUEBMQEOqmv3HuznUpifMiUjh\nU0BMQMPQE+Z0mklEZgAFxASsWlhJIm5s2qfTTCJS+BQQE5BMxFm1sIpNe9SDEJHCp4CYoIZUNS/s\nP07/gCbMiUhhU0BMUGOqhp7+QbYfaI+6FBGRnFJATNDwnV01DiEiBU4BMUELq5LMrSjRfAgRKXgK\niAkyMxpTNWzap4FqESlsOQ0IM7vKzHaY2U4zuy3L/s+Z2Ytm9oKZPW5mSzL2DZjZ5vD1cC7rnKiG\nVDV7jnRxuKM36lJERHImZwFhZnHgTuBqYCVwo5mtHNNsE7DG3S8BHgT+LGNft7uvDl/X5KrOMzE0\nDrFZE+ZEpIDlsgexFtjp7rvdvQ9YD6zLbODuT7h7V7j6LLA4h/VMmosXVVEUMzbqzq4iUsByGRCL\ngH0Z603htpP5OPBoxnrSzDaY2bNmdm22A8zslrDNhpaWlrOveJySiTgrF1YqIESkoOXFILWZ3QSs\nAb6WsXmJu68BPgR83czOHXucu9/t7mvcfU1dXd0UVRtoqK/mhaZW0powJyIFKpcBsR+oz1hfHG4b\nxcyuBL4AXOPuw6O+7r4/fN8NPAk05LDWCWtcUkNX3wA7DmrCnIgUplwGxHPAcjNbZmbFwA3AqKuR\nzKwB+AZBOBzK2F5jZiXhci1wOfBiDmudsIb6oSfMaaBaRApTzgLC3dPArcBjwEvAA+6+zcxuN7Oh\nq5K+BpQDPxhzOeuFwAYz+w3wBHCHu+dVQNTPnkVtebHGIUSkYBXl8sPd/RHgkTHbvpSxfOVJjnsa\nuDiXtZ0tM6MhVaNLXUWkYOXFIPV01ZCqZvfhTo519kVdiojIpFNAnIXG8Alzm3XbDREpQAqIs3DJ\n4irimjAnIgVKAXEWSouLWDG/QlcyiUhBUkCcpYZUNZv3HWdg0KMuRURkUikgzlJjqoaO3jSvHNKE\nOREpLAqIs9SQ0oQ5ESlMCoiztHROKTWlCT1hTkQKjgLiLA1NmNMT5kSk0CggJkFjqpqdhzpo7eqP\nuhQRkUmjgJgEwxPmmtSLEJHCoYCYBJfUVxMzNA4hIgVFATEJykuKOH9ehcYhRKSgKCAmSUOqhk17\njzGoCXMiUiAUEJOkMVVNe0+a3Yc7oi5FRGRSKCAmydCEuY17dJpJRAqDAmKSnFNbRtWshO7sKiIF\nQwExSWIxY3V9tW65ISIFQwExiRpTNbx8qJ22Hk2YE5HpTwExiRpS1bjDC/taoy5FROSsKSAm0epU\nNWZoHEJECoICYhJVJhMsn1vOJgWEiBQABcQka6gP7uzqrglzIjK95TQgzOwqM9thZjvN7LYs+z9n\nZi+a2Qtm9riZLcnYd7OZvRK+bs5lnZOpcUk1x7v6efVwZ9SliIiclZwFhJnFgTuBq4GVwI1mtnJM\ns03AGne/BHgQ+LPw2NnAl4E3AWuBL5tZTa5qnUzDE+Z0uauITHO57EGsBXa6+2537wPWA+syG7j7\nE+7eFa4+CywOl98N/Iu7H3X3Y8C/AFflsNZJc15dORUlRRqHEJFpL5cBsQjYl7HeFG47mY8Dj07k\nWDO7xcw2mNmGlpaWsyx3csRixupUtXoQIjLtjSsgzOzTZlZpgW+b2UYze9dkFWFmNwFrgK9N5Dh3\nv9vd17j7mrq6uskq56w1pGrY0dxGR2866lJERM7YeHsQH3P3NuBdQA3wEeCO0xyzH6jPWF8cbhvF\nzK4EvgBc4+69Ezk2XzWkqhl0eEFPmBORaWy8AWHh+3uAf3D3bRnbTuY5YLmZLTOzYuAG4OFRH2rW\nAHyDIBwOZex6DHiXmdWEg9PvCrdNCw311QC6L5OITGtF42z3vJn9M7AM+LyZVQCDpzrA3dNmdivB\nL/Y4cI+7bzOz24EN7v4wwSmlcuAHZgaw192vcfejZvY/CUIG4HZ3Pzrh7y4i1aXFnFNXpoFqEZnW\nxhsQHwdWA7vdvSu8DPX3TneQuz8CPDJm25cylq88xbH3APeMs76805iq4efbD+HuhOEnIjKtjPcU\n05uBHe5+PBxQ/iKgO9KdQmOqhqOdfew92nX6xiIieWi8AfF3QJeZXQr8Z2AXcG/OqioADalgHEI3\n7hOR6Wq8AZH24OZC64C/cfc7gYrclTX9nT+vgrLiuAaqRWTaGu8YRLuZfZ7g8ta3mFkMSOSurOkv\nHjMura9WD0JEpq3x9iCuB3oJ5kM0E8xLmNCktpmoMVXDSwfa6e4biLoUEZEJG1dAhKFwH1BlZu8D\netxdYxCn0ZCqZmDQNWFORKal8d5q44PAr4HrgA8CvzKzD+SysEKgO7uKyHQ23jGILwBvHJrtbGZ1\nwM8IbtEtJzG7rJilc0o1YU5EpqXxjkHExtwK48gEjp3RGlM1bNyrJ8yJyPQz3l/yPzWzx8zso2b2\nUeCfGDNDWrJrSFVzuKOXpmPdUZciIjIh4zrF5O7/xczeD1webrrb3X+Uu7IKx8g4xDHqZ5dGXI2I\nyPiNdwwCd38IeCiHtRSkFfMrmJUIJsytW32q5yWJiOSXUwaEmbUD2U6eG+DuXpmTqgpIUTzGJYur\nNFAtItPOKccg3L3C3SuzvCoUDuPXuKSGba+30dOvCXMiMn3oSqQp0FBfTXrQ2bpfN8AVkelDATEF\nhgaqdeM+EZlOFBBToK6ihPrZs3TjPhGZVhQQUySYMHdME+ZEZNpQQEyRhvpqDrb1cqC1J+pSRETG\nRQExRRqXjEyYExGZDhQQU2TF/EpKimIaqBaRaUMBMUWKi4IJc+pBiMh0kdOAMLOrzGyHme00s9uy\n7H+rmW00s/TY50uY2YCZbQ5fD+eyzqnSmKph2/42etOaMCci+S9nAWFmceBO4GpgJXCjma0c02wv\n8FHg/iwf0e3uq8PXNbmqcyo1pKrpGxhk2+ttUZciInJauexBrAV2uvtud+8D1gPrMhu4+2vu/gIw\nmMM68oYmzInIdJLLgFgE7MtYbwq3jVfSzDaY2bNmdu3klhaNeZVJFlVrwpyITA/jvt13BJa4+34z\nOwf4uZltcfddmQ3M7BbgFoBUKhVFjRPWkKpWD0JEpoVc9iD2A/UZ64vDbePi7vvD993Ak0BDljZ3\nu/sad19TV1d3dtVOkYZUDfuPd3OwTRPmRCS/5TIgngOWm9kyMysGbgDGdTWSmdWYWUm4XEvwJLsX\nc1bpFGpMVQOwcY9OM4lIfstZQLh7GrgVeAx4CXjA3beZ2e1mdg2Amb3RzJqA64BvmNm28PALgQ1m\n9hvgCeAOdy+IgFi5sJLieIxN+3SaSUTyW07HINz9EeCRMdu+lLH8HMGpp7HHPQ1cnMvaolJSFOei\nRZXqQYhI3tNM6gg0pGrYsr+VvvSMuLpXRKYpBUQEGlM19KYHeemAJsyJSP5SQESgcUkwUL1J8yFE\nJI8pICKwoGoW8yuTbNR8CBHJYwqIiDQuqWbTPvUgRCR/KSAi0lBfw76j3bS090ZdiohIVgqIiGgc\nQkTynQIiIqsWVpGIm8YhRCRvKSAikkzEWbmwSj0IEclbCogINdRX80JTK+kBTZgTkfyjgIhQ45Ia\nuvsH2N7cHnUpIiInUEBEqKFeA9Uikr8UEBFaXDOLuooSDVSLSF5SQETIzGhMVasHISJ5SQERsYZU\nDa8d6eJIhybMiUh+UUBErDFVA8BmPUBIRPKMAiJiFy+qoihmbNRpJhHJMwqIiM0qjnPhgko2aaBa\nRPKMAiIPNKSq+c2+4wwMetSliIgMU0DkgcZUDZ19A7x8UBPmRCR/KCDyQEMqmDCncQgRyScKiDyQ\nml3KnLJiNu7ROISI5A8FRB4wMxpSesKciOSXnAaEmV1lZjvMbKeZ3ZZl/1vNbKOZpc3sA2P23Wxm\nr4Svm3NZJ69vhsFo76jakKphd0snza09kdYhIjIkZwFhZnHgTuBqYCVwo5mtHNNsL/BR4P4xx84G\nvgy8CVgLfNnManJSaMvL8K0r4Qc3Q19nTr7EePz2+XXEDN7+50/ypR9v5dXD0dUiIgK57UGsBXa6\n+2537wPWA+syG7j7a+7+AjD2z/d3A//i7kfd/RjwL8BVOamydjlc+WXY/hP49rvg2J6cfJnTuWhR\nFf/0qbfw3ksWsP7X+3jH/36S3//uBp7ZdQR3Xf4qIlMvlwGxCNiXsd4Ubpu0Y83sFjPbYGYbWlpa\nzqxKM/itP4QP/wCO74Nvvh1ee+rMPussXbigkj+/7lKeuu3t/OHbz+P5PUe58ZvP8r6/foofbmyi\nL60HC4nI1JnWg9Tufre7r3H3NXV1dWf3YeddCX/wc5g1G+5dB899e3KKPANzK5J87l0X8Mzn38n/\n+g8X05se5HMP/IYr/vTn3PnETo519kVWm4jMHLkMiP1Afcb64nBbro89c7XnwR88Due+A/7pc/CT\nz0I6ul/GyUScG9em+OfPvJW//703csH8Cr722A7efMfjfOFHW9jV0hFZbSJS+CxX57fNrAh4GXgn\nwS/354APufu2LG3/HviJuz8Yrs8GngcawyYbgTe4+9GTfb01a9b4hg0bJqf4wQF4/Hb4t6/Dksvh\ng/dCWe3kfPZZ2t7cxj1Pvco/bn6dvvQg71gxl9+/YhlvPncOZhZ1eSIyzZjZ8+6+Juu+XA6Amtl7\ngK8DceAed/+qmd0ObHD3h83sjcCPgBqgB2h291XhsR8D/lv4UV919++c6mtNakAMeeEH8PCtUDYX\nbrwf5l88uZ9/Fg539PK9Z/fwD8/s4UhnHyvmV/DxK5ZxzeqFlBTFoy5PRKaJyAJiKuUkIAD2b4T1\nH4ae43Dt38Gqayf/a5yFnv4Bfrx5P99+6lVePthBbXkJv/vmJXz4TSnmlJdEXZ6I5DkFxNlqPwj/\n9yZo+jW89b/C2z4Psfwa33d3ntp5mG/966v84uUWSopi/IfGRXzs8mUsn1cRdXkikqcUEJMh3Qs/\n+Rxs/h6seB/8+7ugJD9/8b5ysJ17/u1VfrhxP73pQX77/Dp+/y3LuOK8Wo1TiMgoCojJ4g6/ugse\n+wLUng83fh9mL8vt1zwLRzp6ue9Xe7n3mT0c7ujlgnkVfOyKpaxbvYhkQuMUIqKAmHy7noAffDSY\nZHfdd+Gc356ar3uGetMDPLz5db791Ktsb25nTlkxN122hJsuW0JdhcYpRGYyBUQuHNkF6z8Eh1+B\nq/4XrL0lCIw85u48vesI337qVX6+/RDFRTGuXb2Qj19xDhfMz8/TZSKSWwqIXOlpgx/eAi8/Cg0f\ngff+byiaHn+R7zzUwXf+7VUe2thET/8gb1ley8euWMZvL68jFsvvoBORyaOAyKXBQXjiq/Cvfw71\nb4Lrvwflc6e+jjN0rLOP+3+9l+8+/RqH2ns5b245N70pxepUDefPK6e0uCjqEkUkhxQQU2HrD+Ef\n/xOUzoYb7oOFDdHVcgb60oP85IVgnGLb621AcMZs6ZwyLphXwYoFFayYX8mFCyqorylVL0OkQCgg\npsqB38D3PwRdh2HdnXDxB05/TJ5xd/Ye7WJ7czvbD7SzvbmN7c3tvHakk6H/VUqL41wwv4IV84PQ\nGHqvKk1EW7yITJgCYip1tMADH4G9z8AVn4V3/HeITf9LSrv60rx8sIMdzW28lBEcx7v6h9ssrEoG\nwbEgCI0LF1SyrLaMRDy/JhWKyAgFxFRL98Gj/wWe/3tY/m54/7cgWRl1VZPO3TnY1stLzW3saG5n\n+4EgNHYe6iA9GPx/VRyPce7cci6cP3KaasWCCurKSzRpTyQPKCCi4A7PfQse/WOYc14wqW7OuVFX\nNSX60oPsaukY7mUMnao62NY73GZOWXF4mioIjAvnV7J8Xrkm8IlMMQVElF79JTxwM/gAfOA7cN47\no64oMkc7+4LQONAe9Dia29hxsJ2e/uBJeTGDpbVlXBiOa5w7t5z5VUnmVyaZW1FCkU5ViUw6BUTU\njr0WDF63vATv+gpc9p/yflLdVBkYdPYc6Qx7GmGPo7mdvUe7RrUzg9ryEhZUJZlXGYTGUHjMH9pW\nlaS8RJflikyEAiIf9HbAj/4jbP8JXPoheN//gUQy6qryVkdvmj1HOjnY1sOB1h4OtvbQ3NZDc1sv\nza3dNLf20NaTPuG4ipIi5oXBMa8yGQTKUJBUJplXVUJtWYku0xUJnSog9OfWVCkphw/+A/ziT+EX\nd8Dhl4P5EhXzo64sL5WXFLFqYRWrFladtE1XX5qDbb00t/bQ3NZNc2svB9t6aG7t4UBbDzt3HuZQ\new+DY/4GSsSNuRVJ5lWWhL2QWcyvKhnumSyomsXcyhKNh8iMpx5EFF78MfzoE5CsCkJi0Ruirqhg\nDQw6hzuCEDnQ2hMESFvQI8lc7+obOOHYmtIE8yqT1FWUUDUrQdWsBNWlCapnFQfrpQmqh9+LqS5N\nKFRk2tEppnzUvBXW3xg8jOiav4ZLr4+6ohnL3WnvTQc9kdaRAGkOeyNHOvto6+7neHc/rd39DIzt\nkmQoLopRHQZJECrFw8tD2ytnJaguLQ7CJdxWkUwQ12kviYBOMeWj+RfBHzwJD/wu/OgWOLgFrvwf\nBTGpbroxMyqTCSqTCc4/zdP33J2O3jTHu4KwaO3u53hXP8e7+4L1rtHb9h/v5sXXWzne3Z+1lzJS\nQzB+Ul1anBEuiYxwKaZyVhGVySBMKmcVBe/J4L24SFd4yeRTQESpbA787j/CT2+Dp/866FU03ATz\nLw7mTigs8o6ZURH+kq6f4LF96cEwVPpGQmQoULr7g15KV99wT2X/se5x9VoAkolYGB5FVM5KjAqP\noWCpHN53YtCUFcc1cVFOoICIWjwR3CZ83kXw08/D7ieC7UVJmLsy6GnMuzgIjXmrCnJG9kxRXBSj\nrqJkwg9pGuq1tPWkaevup33ovbeftu6h5Yx9PUGoNB3toq0naNM3MHjKrxGPGeUlRRm9lOy9lcpk\ngrKSIspK4pSXFFFaXER5uF5WUkRJUUxBU0A0BpFP0n1weAc0bwl6Ewe3BMvdx0baVC8JwmL+xUGo\nzL8o2KZ/lHIKPf0Dw+ExFDCZy0P7Mpcz97X3nnhJcTbxmFFWHA9DJHyF60GgxMNAGb1cVhKnrDjj\nmDCAZiXUs8k1jUFMF0XFI7/8h7hD2+twcCs0vxAGx1bY/k9AGO4lVUHvYv5FI8Ex90JIzIrk25D8\nk0zESSbiZ/yI2YFBpyMMjs6+NJ29aTp7B+jsTdPRm6arb4CO3mB75nJnX9DmaGdXeFyw3ps+dY9m\niBlhcIwEyFCwJBNxSopilCTiJBOx4HssilOSiJEsig1/z8lE0KakaKTNUPuSjHa6SOBEOQ0IM7sK\n+EsgDnzL3e8Ys78EuBd4A3AEuN7dXzOzpcBLwI6w6bPu/olc1pq3zKBqUfA6/90j2/s64eCLYS9j\na9DT2HQf9HeGx8VgzvIwcDJOU1XMi+b7kGktHjOqShOTdkv3/oFBunoHhsNmbMgMBcuoIOpL0xWu\nN7f10NM/QE//IL3pQXr7B+hJD9A/cOZnRBJxGw6YkowQGQ6ZoW1F8eFQGtUuI2yGgqdkbHCNaZfv\noZSzgDCzOHAn8DtAE/CcmfWgVVsAAApOSURBVD3s7i9mNPs4cMzdzzOzG4A/BYau99zl7qtzVd+0\nV1wG9W8MXkMGB+HYq0FYHNwaBMfeZ2HrgyNtyurCU1MZp6lqlwdjISJTJBGPUVUam/RniAwMOj39\nA/SmB8MAGQqR4L0nPRCESea2LG16+gfoHdOmtbv/hON6+wdPO75zKom4DYfM2FAa6d2MhNKobRlh\nM68yydtXTP6TLHPZg1gL7HT33QBmth5YB2QGxDrgT8LlB4G/MZ1wPHOxWHDH2DnnwqprR7Z3HYWD\n28LQCMc1fnUXDPQF++PFwSmpeWFvo24FlM8LwqR0tq6mkmkjHrNwDGPqvubAoNM3FEijQmd08PSm\nR7aNDrHRoZS5r62nf3SIhfvGhlJDqnraBcQiYF/GehPwppO1cfe0mbUCc8J9y8xsE9AGfNHd/3Xs\nFzCzW4BbAFKp1ORWX0hKZ8OytwSvIQP9we0+MgfDX/4pbP7emIMtOL60NgiMsjnhex2UDi3XjmxL\nVgdBJTJDxGPGrOI4s4qn7g+pgUEf1ZPJ1Z/V+TpIfQBIufsRM3sD8I9mtsrd2zIbufvdwN0QXMUU\nQZ3TVzwRDGzPW8XwWT136DgIh1+BzhboOhK8d7ZA5+Hgdegl6Pzl6CurMlk8DI7akeAYGy7D67XB\n7UbUaRSZkHjMKC0uorQ4t18nlwGxH0bNJVocbsvWpsnMioAq4IgH1972Arj782a2CzgfmObXseY5\ns+DmgeO5geBAf3DqqrMleAb3UIAMBcpQuLy+Odje25r9c2KJIChKMwJlKFxmzQ4CJFkV9EySlSPr\nRVN4DkFkhsplQDwHLDezZQRBcAPwoTFtHgZuBp4BPgD83N3dzOqAo+4+YGbnAMuB3TmsVSYqngiu\niBrvVVHp3oweSRgmXRmB0hnuO/ZqsK+v49SfV5SEkozASFaNDpBkVbi/Ovv+RKl6LiKnkbOACMcU\nbgUeI7jM9R5332ZmtwMb3P1h4NvAP5jZTuAoQYgAvBW43cz6gUHgE+5+NFe1yhQoKoHKhcFrPPq7\ngx5Kbxv0tIavNug5PrI+al8rHN87sjzQe+rPt/gpwqV6dPiUVARXjRWXB7dtH1ouLg/mrogUKM2k\nlsLU33NigJwsWHraTtx/uh7MkFgiCIzhEMkIj+KyEwPlhKApg+Ixx8bzdWhQCpFmUsvMk0gGr/Iz\nvPRvIB0GyfHgaYB9nUFo9HWcuN7XGbx620eWu46M7OvtgHT3+L92UTJ70BTNCnpiRckJvA8tl5y8\nTbxEV55JVgoIkWziReHlvbMn5/MGB04eJplB05sZOhnvPW2QPgTpnmA8J90T3Lsr3XP602njES+e\nYPBkvo8JrkS2IDtFm1iRxoPylAJCZCrEMsY8JtvgYDDpcVR4nOz9NG0GerNv720PLiLoDwOpP2P/\n2QaUxTJ6OxmhkkieOoziiSDY4sVnsZw4eRtNEFVAiEx7sRjEwl+oURgcDINlTKj0d58koDLW+7Ns\nG7ve1xVcsDDqs7qD04ADfTDYn5vvy2InCZSx20qCixWKkmFPrCTjPeP03th9426fsW2Ke1sKCBE5\nO7EYxGZFd/dg92BezkBf+Mq2fLr9p1kePMnnpHuD5Z42SLeM9MAG+jLee2BwfLdLPz3LCI2MUFq4\nGj5wzyR9jREKCBGZ3szCX5Z5fMnxcC8rIzTSfScGSuZpvrEhc0L7jG3VS3JStgJCRCTXou5lnSFd\n2yYiIlkpIEREJCsFhIiIZKWAEBGRrBQQIiKSlQJCRESyUkCIiEhWCggREcmqYJ4HYWYtwJ6z+Iha\n4PAklTPd6Wcxmn4eo+nnMaIQfhZL3L0u246CCYizZWYbTvbQjJlGP4vR9PMYTT+PEYX+s9ApJhER\nyUoBISIiWSkgRtwddQF5RD+L0fTzGE0/jxEF/bPQGISIiGSlHoSIiGSlgBARkaxmfECY2VVmtsPM\ndprZbVHXEyUzqzezJ8zsRTPbZmafjrqmqJlZ3Mw2mdlPoq4lamZWbWYPmtl2M3vJzN4cdU1RMrPP\nhv9OtprZ980sooeC586MDggziwN3AlcDK4EbzWxltFVFKg38Z3dfCVwGfHKG/zwAPg28FHUReeIv\ngZ+6+wrgUmbwz8XMFgGfAta4+0VAHLgh2qom34wOCGAtsNPdd7t7H7AeWBdxTZFx9wPuvjFcbif4\nBbAo2qqiY2aLgfcC34q6lqiZWRXwVuDbAO7e5+7Ho60qckXALDMrAkqB1yOuZ9LN9IBYBOzLWG9i\nBv9CzGRmS4EG4FfRVhKprwP/FRiMupA8sAxoAb4TnnL7lpmVRV1UVNx9P/DnwF7gANDq7v8cbVWT\nb6YHhGRhZuXAQ8Bn3L0t6nqiYGbvAw65+/NR15InioBG4O/cvQHoBGbsmJ2Z1RCcbVgGLATKzOym\naKuafDM9IPYD9Rnri8NtM5aZJQjC4T53/2HU9UTocuAaM3uN4NTjO8zse9GWFKkmoMndh3qUDxIE\nxkx1JfCqu7e4ez/wQ+C3Iq5p0s30gHgOWG5my8ysmGCQ6eGIa4qMmRnBOeaX3P0voq4nSu7+eXdf\n7O5LCf6/+Lm7F9xfiOPl7s3APjO7INz0TuDFCEuK2l7gMjMrDf/dvJMCHLQvirqAKLl72sxuBR4j\nuArhHnffFnFZUboc+Aiwxcw2h9v+m7s/EmFNkj/+ELgv/GNqN/B7EdcTGXf/lZk9CGwkuPpvEwV4\n2w3dakNERLKa6aeYRETkJBQQIiKSlQJCRESyUkCIiEhWCggREclKASGSA2b2Nt0BVqY7BYSIiGSl\ngJAZy8xuMrNfm9lmM/tGePt3zKzDzP5PeK//x82sLty+2syeNbMXzOxH4f14MLPzzOxnZvYbM9to\nZueGX6I84/kJ94UzbsfW8KSZ/WlYx8tm9pZw+0fN7G8y2v3EzN6WUd/Xwvp+ZmZrw8/ZbWbX5Pan\nJjOJAkJmJDO7ELgeuNzdVwMDwIfD3WXABndfBfwC+HK4/V7gj939EmBLxvb7gDvd/VKC+/EcCLc3\nAJ8heNbIOQQz1bMpcve1Ydsvn6RNpjKCW3+sAtqBrwC/A/x74PZxHC8yLjP6Vhsyo70TeAPwXPiH\n/SzgULhvEPi/4fL3gB+Gz0OodvdfhNu/C/zAzCqARe7+IwB37wEIP/PX7t4Urm8GlgJPZall6KaI\nz4dtTqcP+Gm4vAXodfd+M9syzuNFxkUBITOVAd9198+Po+2Z3o+mN2N5gJP/e+vN0ibN6B5+5uMs\n+33kHjmDQ8e7+2D48BqRSaFTTDJTPQ58wMzmApjZbDNbEu6LAR8Ilz8EPOXurcCxoTECgpsa/iJ8\n8l6TmV0bfk6JmZVOQn2vAavNLGZm9QRPPxSZUvprQ2Ykd3/RzL4I/LOZxYB+4JPAHoKH4awN9x8i\nGKsAuBm4KwyAzLuZfgT4hpndHn7OdZNQ4r8BrxLcUvslgruGikwp3c1VZAwz63D38qjrEImaTjGJ\niEhW6kGIiEhW6kGIiEhWCggREclKASEiIlkpIEREJCsFhIiIZPX/AS5+yMph9zwjAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h2y5i6l3eR0i",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_2,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "train(model, criterion, metric, optimizer, EPOCHS_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oQI8sgvyBeDr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "745669ac-f122-42b6-a3e3-8761192a8c7a"
      },
      "source": [
        "with open(PATH_DATASET_TEST, 'r') as file:\n",
        "    dataset = file.readlines()\n",
        "    dataset = [' '.join(t.split('\\n')) for t in dataset]\n",
        "    dataset = [' '.join(t.split()) for t in dataset]\n",
        "\n",
        "for text in dataset:\n",
        "    prepare_test(text)\n",
        "\n",
        "'''\n",
        "res = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    while len(dataset) > 0:\n",
        "        texts, dataset = dataset[:BATCH_SIZE], dataset[BATCH_SIZE:]\n",
        "        texts, masks = text_collate_fn(texts)\n",
        "        texts = texts.to(device)\n",
        "        masks = texts.to(device)\n",
        "\n",
        "        scores = model(texts, masks)\n",
        "        scores = scores.cpu().numpy()\n",
        "        scores = np.around(scores).astype(np.int32) + 1\n",
        "        res += scores.tolist()\n",
        "\n",
        "res = '\\n'.join([str(x) for x in res])\n",
        "with open(PATH_RESULTS, 'w') as file:\n",
        "    file.write(res)\n",
        "'''"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e4670dcb7018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprepare_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m '''\n",
            "\u001b[0;32m<ipython-input-40-311e43cba8da>\u001b[0m in \u001b[0;36mprepare_test\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}