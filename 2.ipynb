{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "PATH_DATASET_TEXTS = os.path.join('data', 'texts_train.txt')\n",
    "PATH_DATASET_SCORES = os.path.join('data', 'scores_train.txt')\n",
    "\n",
    "MAX_TEXT_LEN = 200\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001\n",
    "W_L2_NORM = 0.0\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATASET_TEXTS, 'r') as file:\n",
    "    texts = file.readlines()\n",
    "    \n",
    "texts = [text[:-1] for text in texts]\n",
    "for c in '.,!?\"()':\n",
    "    texts = [text.replace(c, f' {c} ') for text in texts]\n",
    "texts = [' '.join(text.split()) for text in texts]\n",
    "    \n",
    "    \n",
    "with open(PATH_DATASET_SCORES, 'r') as file:\n",
    "    scores = file.readlines()\n",
    "    \n",
    "scores = [int(s) - 1 for s in scores]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    texts, scores, test_size=0.1)\n",
    "train_data = list(zip(x_train, y_train))\n",
    "val_data = list(zip(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    do_lower_case=False\n",
    ")\n",
    "\n",
    "def collate_fn(data):\n",
    "    texts, scores = zip(*data)\n",
    "    \n",
    "    texts = [tokenizer.tokenize(text) for text in texts]\n",
    "    texts = [text[:MAX_TEXT_LEN - 2] for text in texts]\n",
    "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
    "    texts = [[tokenizer.cls_token_id] + text + [tokenizer.sep_token_id] for text in texts]\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
    "    texts = [text + [tokenizer.pad_token_id] * (max_len - len(text)) for text in texts]\n",
    "    texts = torch.LongTensor(texts)\n",
    "    masks = torch.LongTensor(masks)\n",
    "    \n",
    "    scores = torch.LongTensor(scores)\n",
    "    \n",
    "    return texts, masks, scores\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        layers = [\n",
    "            nn.Linear(768, n_classes)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        x = self.bert(text, attention_mask=mask)[1]\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epochs):\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for texts, masks, scores in train_data_loader:\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "            scores = scores.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ps = model(texts, masks)\n",
    "            loss = criterion(ps, scores)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_train.append(np.array(losses).mean())\n",
    "\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for texts, masks, scores in val_data_loader:\n",
    "                texts = texts.to(device)\n",
    "                masks = masks.to(device)\n",
    "                scores = scores.to(device)\n",
    "                \n",
    "                ps = model(texts, masks)\n",
    "                loss = criterion(ps, scores)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        losses_val.append(np.array(losses).mean())\n",
    "\n",
    "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(n_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "\n",
    "train(model, criterion, optimizer, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
