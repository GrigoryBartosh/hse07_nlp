{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/hse07_nlp/blob/master/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-12U8rg1TJL"
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVy7cjtt1TJR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "PATH_LOGS = os.path.join('data', 'logs_tf')\n",
    "\n",
    "PATH_DATASET = os.path.join('data', 'train_qa.csv')\n",
    "\n",
    "PATH_DATASET_TEST = os.path.join('data', 'test.txt')\n",
    "PATH_RESULTS = os.path.join('data', 'results.txt')\n",
    "\n",
    "MAX_TEXT_LEN = 256\n",
    "\n",
    "EPOCHS_1 = 1\n",
    "EPOCHS_2 = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE_1 = 0.0001\n",
    "LEARNING_RATE_2 = 0.0001\n",
    "W_L2_NORM = 0.0\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOec3Kj71TJV"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(PATH_DATASET)\n",
    "dataset_texts = dataset['paragraph']\n",
    "dataset_questions = dataset['question']\n",
    "dataset_answers = dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHk4_axm1TJZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9319rsQ1TJc"
   },
   "outputs": [],
   "source": [
    "def prepare_sample(text, question, answer):\n",
    "    answer = answer.lower()\n",
    "    while (answer[0] == '.'):\n",
    "        answer = answer[1:]\n",
    "    while (answer[-1] in ['.', '?']):\n",
    "        answer = answer[:-1]\n",
    "        \n",
    "    if answer not in text.lower():\n",
    "        return [], []\n",
    "    \n",
    "    first = text.lower().find(answer)\n",
    "    last = first + len(answer)\n",
    "    \n",
    "    text_1 = text[:first].strip()\n",
    "    text_2 = text[first:last].strip()\n",
    "    text_3 = text[last:].strip()\n",
    "    text_tokens = tokenizer.tokenize(text_1)\n",
    "    first = len(text_tokens)\n",
    "    text_tokens += tokenizer.tokenize(text_2)\n",
    "    last = len(text_tokens) - 1\n",
    "    text_tokens += tokenizer.tokenize(text_3)\n",
    "    \n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "    if len(text_tokens) > length:\n",
    "        part_length = length // 3\n",
    "        stride = 3 * part_length\n",
    "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "        indexes = indexes.astype(np.int32)\n",
    "\n",
    "        max_index = indexes.max()\n",
    "        diff = max_index + 1 - len(text_tokens)\n",
    "        text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "        text_tokens = np.array(text_tokens)[indexes].tolist()\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for i, ts in enumerate(text_tokens):\n",
    "            while ts[-1] == tokenizer.pad_token:\n",
    "                ts = ts[:-1]\n",
    "                \n",
    "            tokens += [ts]\n",
    "                \n",
    "            lfirst = first - i * part_length\n",
    "            llast = last - i * part_length\n",
    "            \n",
    "            mask = lfirst >= 0 and lfirst < len(ts) and llast >= 0 and llast < len(ts)\n",
    "            labels += [((lfirst if mask else 0, mask), (llast if mask else 0, mask))]\n",
    "    else:\n",
    "        tokens = [text_tokens]\n",
    "        labels = [((first, 1), (last, 1))]\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = [tokenizer.cls_token] + \\\n",
    "                    question_tokens + \\\n",
    "                    [tokenizer.sep_token] + \\\n",
    "                    tokens[i] + \\\n",
    "                    [tokenizer.sep_token]\n",
    "        labels[i] = ((labels[i][0][0] + 2 + len(question_tokens), labels[i][0][1]),\n",
    "                     (labels[i][1][0] + 2 + len(question_tokens), labels[i][1][1]))\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-47IcN861TJg"
   },
   "outputs": [],
   "source": [
    "dataset_tokens, dataset_labels = [], []\n",
    "\n",
    "for text, question, answer in tqdm(list(zip(dataset_texts, dataset_questions, dataset_answers))):\n",
    "    tokens, labels = prepare_sample(text, question, answer)\n",
    "    dataset_tokens += tokens\n",
    "    dataset_labels += labels\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset_tokens, dataset_labels, test_size=0.1)\n",
    "train_data = list(zip(x_train, y_train))\n",
    "val_data = list(zip(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPcZDQkT1TJj"
   },
   "outputs": [],
   "source": [
    "def text_collate_fn(texts):\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
    "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
    "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
    "    texts = torch.LongTensor(texts)\n",
    "    masks = torch.LongTensor(masks)\n",
    "\n",
    "    return texts, masks\n",
    "\n",
    "def collate_fn(data):\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    texts, masks = text_collate_fn(texts)\n",
    "    \n",
    "    labels_first, labels_last = zip(*labels)\n",
    "    labels_first_pos, labels_first_valid = zip(*labels_first)\n",
    "    labels_last_pos, labels_last_valid = zip(*labels_last)\n",
    "    \n",
    "    labels_first_pos = torch.LongTensor(labels_first_pos)\n",
    "    labels_first_mask = torch.LongTensor(labels_first_valid)\n",
    "    labels_last_pos = torch.LongTensor(labels_last_pos)\n",
    "    labels_last_mask = torch.LongTensor(labels_last_valid)\n",
    "    \n",
    "    return texts, masks, labels_first_pos, labels_first_mask, labels_last_pos, labels_last_mask\n",
    "\n",
    "def infinit_data_loader(data_loader):\n",
    "    while True:\n",
    "        for x in data_loader:\n",
    "            yield x\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_data_loader = infinit_data_loader(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IFl_5gL1TJn"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(16, 2)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def masked_softmax(self, vec, mask, dim=1):\n",
    "        masked_vec = vec * mask.float()\n",
    "        max_vec = torch.max(masked_vec, dim=dim, keepdim=True)[0]\n",
    "        exps = torch.exp(masked_vec - max_vec)\n",
    "        masked_exps = exps * mask.float()\n",
    "        masked_sums = masked_exps.sum(dim, keepdim=True)\n",
    "        zeros = (masked_sums == 0)\n",
    "        masked_sums += zeros.float()\n",
    "        return masked_exps / masked_sums\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        x = self.bert(text, attention_mask=mask)[0]\n",
    "        x = self.layers(x)\n",
    "        x = self.masked_softmax(x, mask[:, :, None])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6H8Vd4R1TJr"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    EPS  = 1e-8\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, output, output_mask, target, target_mask):\n",
    "        sm_0 = -torch.log(1 - output + MaskedLoss.EPS)\n",
    "        sm_0_mask = output_mask * target_mask[:, None]\n",
    "        sm_0 = sm_0 * sm_0_mask\n",
    "        \n",
    "        sm_1 = torch.gather(output, 1, target[:, None]).squeeze()\n",
    "        sm_1 = -torch.log(sm_1 + MaskedLoss.EPS)\n",
    "        sm_1 = sm_1 * target_mask\n",
    "        \n",
    "        loss = sm_0.sum() / sm_0_mask.sum() + sm_1.sum() / target_mask.sum()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def get_summary_writer():\n",
    "    name = str(datetime.datetime.now())[:19]\n",
    "    make_dir(PATH_LOGS)\n",
    "    logs_path = os.path.join(PATH_LOGS, name)\n",
    "    return SummaryWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sx8Mkluo1TJu"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epochs):\n",
    "    summary_writer = get_summary_writer()\n",
    "    step = 0\n",
    "    last_val = 0\n",
    "    \n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for texts, masks, lfp, lfm, llp, llm in train_data_loader:\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "            lfp = lfp.to(device)\n",
    "            lfm = lfm.to(device)\n",
    "            llp = llp.to(device)\n",
    "            llm = llm.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ps = model(texts, masks)\n",
    "            loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                   criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += len(texts)\n",
    "            last_val += len(texts)\n",
    "            summary_writer.add_scalar('Train/loss', loss.item(), step)\n",
    "            if last_val >= 10 * BATCH_SIZE:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    texts, masks, lfp, lfm, llp, llm = next(val_data_loader)\n",
    "                    texts = texts.to(device)\n",
    "                    masks = masks.to(device)\n",
    "                    lfp = lfp.to(device)\n",
    "                    lfm = lfm.to(device)\n",
    "                    llp = llp.to(device)\n",
    "                    llm = llm.to(device)\n",
    "\n",
    "                    ps = model(texts, masks)\n",
    "                    loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                           criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "                    summary_writer.add_scalar('Validation/loss', loss.item(), step)\n",
    "                    \n",
    "                model.train()\n",
    "                last_val = 0\n",
    "                \n",
    "        if scheduler:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pi4TSt51TJx"
   },
   "outputs": [],
   "source": [
    "model = TextClassifier()\n",
    "model.to(device)\n",
    "\n",
    "criterion = MaskedLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_1,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
    "\n",
    "train(model, criterion, optimizer, scheduler, EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odHkHiwg3qy_"
   },
   "outputs": [],
   "source": [
    "#train(model, criterion, optimizer, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr62wfbn1TJ4"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_2,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
    "\n",
    "train(model, criterion, optimizer, scheduler, EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(\n",
    "#    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#    0.00001,\n",
    "#    weight_decay=W_L2_NORM\n",
    "#)\n",
    "\n",
    "#train(model, criterion, optimizer, scheduler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ic07H3yj42z3"
   },
   "outputs": [],
   "source": [
    "def get_best(ps):\n",
    "    n = len(ps)\n",
    "    first, last, mx = 0, 0, 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            lmx = ps[i, 0] * ps[j, 1]\n",
    "            if mx < lmx:\n",
    "                mx, first,last = lmx, i, j\n",
    "                \n",
    "    return first, last, mx\n",
    "\n",
    "with open(PATH_DATASET_TEST, 'r') as file:\n",
    "    dataset_test = file.readlines()[1:]\n",
    "    \n",
    "res = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(dataset_test):\n",
    "        _, question_id, paragraph, question = sample.split('\\t')\n",
    "\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        text_tokens = tokenizer.tokenize(paragraph)\n",
    "        \n",
    "        all_tokens = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     text_tokens + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "        length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "        if (len(text_tokens) > length):\n",
    "            part_length = length // 3\n",
    "            stride = 3 * part_length\n",
    "            nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "            indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "            indexes = indexes.astype(np.int32)\n",
    "\n",
    "            max_index = indexes.max()\n",
    "            diff = max_index + 1 - len(text_tokens)\n",
    "            text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "            text_tokens = np.array(text_tokens)[indexes].tolist()\n",
    "\n",
    "            first, last, mx = 0, 0, 0\n",
    "            for i, ts in enumerate(text_tokens):\n",
    "                while ts[-1] == tokenizer.pad_token:\n",
    "                    ts = ts[:-1]\n",
    "\n",
    "                ts = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     ts + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "                texts, masks = text_collate_fn([ts])\n",
    "                texts = texts.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                lps = model(texts, masks)[0]\n",
    "                lps = lps[2 + len(question_tokens):]\n",
    "                lps[:, 0] = F.softmax(lps[:, 0], 0)\n",
    "                lps[:, 1] = F.softmax(lps[:, 1], 0)\n",
    "                lps = lps.cpu().numpy()\n",
    "                \n",
    "                lfirst, llast, lmx = get_best(lps)\n",
    "                if mx < lmx:\n",
    "                    mx = lmx\n",
    "                    first = lfirst + i * part_length\n",
    "                    last = llast + i * part_length\n",
    "                \n",
    "            first += 2 + len(question_tokens)\n",
    "            last += 2 + len(question_tokens)\n",
    "                    \n",
    "        else:\n",
    "            texts, masks = text_collate_fn([all_tokens])\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            ps = model(texts, masks)[0]\n",
    "            ps[:, 0] = F.softmax(ps[:, 0], 0)\n",
    "            ps[:, 1] = F.softmax(ps[:, 1], 0)\n",
    "            ps = ps.cpu().numpy()\n",
    "            \n",
    "            first, last, _ = get_best(ps)\n",
    "            \n",
    "        s = ''\n",
    "        tokens = all_tokens[first:last + 1]\n",
    "        for token in tokens:\n",
    "            if token == tokenizer.unk_token:\n",
    "                continue\n",
    "            \n",
    "            if token[0] == '#':\n",
    "                s += token.replace('#', '')\n",
    "            else:\n",
    "                s += ' ' + token\n",
    "            \n",
    "        res += [question_id + '\\t' + s.strip()]\n",
    "        \n",
    "res = '\\n'.join(res)\n",
    "with open(PATH_RESULTS, 'w') as file:\n",
    "    file.write(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
