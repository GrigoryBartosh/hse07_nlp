{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/hse07_nlp/blob/master/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-12U8rg1TJL"
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVy7cjtt1TJR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "PATH_LOGS = os.path.join('data', 'logs_tf')\n",
    "\n",
    "PATH_DATASET = os.path.join('data', 'train_qa.csv')\n",
    "\n",
    "PATH_DATASET_TEST = os.path.join('data', 'test.txt')\n",
    "PATH_RESULTS = os.path.join('data', 'results.txt')\n",
    "\n",
    "MAX_TEXT_LEN = 256\n",
    "\n",
    "EPOCHS_1 = 1\n",
    "EPOCHS_2 = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE_1 = 0.0001\n",
    "LEARNING_RATE_2 = 0.0001\n",
    "W_L2_NORM = 0.0\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOec3Kj71TJV"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(PATH_DATASET)\n",
    "dataset_texts = dataset['paragraph']\n",
    "dataset_questions = dataset['question']\n",
    "dataset_answers = dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHk4_axm1TJZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9319rsQ1TJc"
   },
   "outputs": [],
   "source": [
    "def prepare_sample(text, question, answer):\n",
    "    answer = answer.lower()\n",
    "    while (answer[0] == '.'):\n",
    "        answer = answer[1:]\n",
    "    while (answer[-1] in ['.', '?']):\n",
    "        answer = answer[:-1]\n",
    "        \n",
    "    if answer not in text.lower():\n",
    "        return [], []\n",
    "    \n",
    "    first = text.lower().find(answer)\n",
    "    last = first + len(answer)\n",
    "    \n",
    "    text_1 = text[:first].strip()\n",
    "    text_2 = text[first:last].strip()\n",
    "    text_3 = text[last:].strip()\n",
    "    text_tokens = tokenizer.tokenize(text_1)\n",
    "    first = len(text_tokens)\n",
    "    text_tokens += tokenizer.tokenize(text_2)\n",
    "    last = len(text_tokens) - 1\n",
    "    text_tokens += tokenizer.tokenize(text_3)\n",
    "    \n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "    if len(text_tokens) > length:\n",
    "        part_length = length // 3\n",
    "        stride = 3 * part_length\n",
    "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "        indexes = indexes.astype(np.int32)\n",
    "\n",
    "        max_index = indexes.max()\n",
    "        diff = max_index + 1 - len(text_tokens)\n",
    "        text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "        text_tokens = np.array(text_tokens)[indexes].tolist()\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for i, ts in enumerate(text_tokens):\n",
    "            while ts[-1] == tokenizer.pad_token:\n",
    "                ts = ts[:-1]\n",
    "                \n",
    "            tokens += [ts]\n",
    "                \n",
    "            lfirst = first - i * part_length\n",
    "            llast = last - i * part_length\n",
    "            \n",
    "            mask = lfirst >= 0 and lfirst < len(ts) and llast >= 0 and llast < len(ts)\n",
    "            labels += [((lfirst if mask else 0, mask), (llast if mask else 0, mask))]\n",
    "    else:\n",
    "        tokens = [text_tokens]\n",
    "        labels = [((first, 1), (last, 1))]\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = [tokenizer.cls_token] + \\\n",
    "                    question_tokens + \\\n",
    "                    [tokenizer.sep_token] + \\\n",
    "                    tokens[i] + \\\n",
    "                    [tokenizer.sep_token]\n",
    "        labels[i] = ((labels[i][0][0] + 2 + len(question_tokens), labels[i][0][1]),\n",
    "                     (labels[i][1][0] + 2 + len(question_tokens), labels[i][1][1]))\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-47IcN861TJg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d651cae6a5a54ea88e80dffc1dd421b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50364), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_tokens, dataset_labels = [], []\n",
    "\n",
    "for text, question, answer in tqdm(list(zip(dataset_texts, dataset_questions, dataset_answers))):\n",
    "    tokens, labels = prepare_sample(text, question, answer)\n",
    "    dataset_tokens += tokens\n",
    "    dataset_labels += labels\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset_tokens, dataset_labels, test_size=0.1)\n",
    "train_data = list(zip(x_train, y_train))\n",
    "val_data = list(zip(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPcZDQkT1TJj"
   },
   "outputs": [],
   "source": [
    "def text_collate_fn(texts):\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
    "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
    "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
    "    texts = torch.LongTensor(texts)\n",
    "    masks = torch.LongTensor(masks)\n",
    "\n",
    "    return texts, masks\n",
    "\n",
    "def collate_fn(data):\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    texts, masks = text_collate_fn(texts)\n",
    "    \n",
    "    labels_first, labels_last = zip(*labels)\n",
    "    labels_first_pos, labels_first_valid = zip(*labels_first)\n",
    "    labels_last_pos, labels_last_valid = zip(*labels_last)\n",
    "    \n",
    "    labels_first_pos = torch.LongTensor(labels_first_pos)\n",
    "    labels_first_mask = torch.LongTensor(labels_first_valid)\n",
    "    labels_last_pos = torch.LongTensor(labels_last_pos)\n",
    "    labels_last_mask = torch.LongTensor(labels_last_valid)\n",
    "    \n",
    "    return texts, masks, labels_first_pos, labels_first_mask, labels_last_pos, labels_last_mask\n",
    "\n",
    "def infinit_data_loader(data_loader):\n",
    "    while True:\n",
    "        for x in data_loader:\n",
    "            yield x\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_data_loader = infinit_data_loader(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IFl_5gL1TJn"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(16, 2)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def masked_softmax(self, vec, mask, dim=1):\n",
    "        masked_vec = vec * mask.float()\n",
    "        max_vec = torch.max(masked_vec, dim=dim, keepdim=True)[0]\n",
    "        exps = torch.exp(masked_vec - max_vec)\n",
    "        masked_exps = exps * mask.float()\n",
    "        masked_sums = masked_exps.sum(dim, keepdim=True)\n",
    "        zeros = (masked_sums == 0)\n",
    "        masked_sums += zeros.float()\n",
    "        return masked_exps / masked_sums\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        x = self.bert(text, attention_mask=mask)[0]\n",
    "        x = self.layers(x)\n",
    "        x = self.masked_softmax(x, mask[:, :, None])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6H8Vd4R1TJr"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    EPS  = 1e-8\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, output, output_mask, target, target_mask):\n",
    "        sm_0 = -torch.log(1 - output + MaskedLoss.EPS)\n",
    "        sm_0_mask = output_mask * (1 - target_mask[:, None])\n",
    "        sm_0 = sm_0 * sm_0_mask\n",
    "        sm_0_mask = sm_0_mask.sum(dim=1)\n",
    "        sm_0 = sm_0.sum(dim=1) / torch.max(sm_0_mask, torch.ones_like(sm_0_mask))\n",
    "        \n",
    "        sm_1 = torch.gather(output, 1, target[:, None]).squeeze()\n",
    "        sm_1 = -torch.log(sm_1 + MaskedLoss.EPS)\n",
    "        sm_1 = sm_1 * target_mask\n",
    "        \n",
    "        print(sm_0)\n",
    "        print(sm_1)\n",
    "        print(target_mask)\n",
    "        print()\n",
    "        one = torch.ones_like(target_mask.sum())\n",
    "        loss = sm_0.sum() / torch.max((1 - target_mask).sum(), one) + \\\n",
    "               sm_1.sum() / torch.max(target_mask.sum(), one)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def get_summary_writer():\n",
    "    name = str(datetime.datetime.now())[:19]\n",
    "    make_dir(PATH_LOGS)\n",
    "    logs_path = os.path.join(PATH_LOGS, name)\n",
    "    return SummaryWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sx8Mkluo1TJu"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epochs):\n",
    "    summary_writer = get_summary_writer()\n",
    "    step = 0\n",
    "    last_val = 0\n",
    "    \n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for texts, masks, lfp, lfm, llp, llm in train_data_loader:\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "            lfp = lfp.to(device)\n",
    "            lfm = lfm.to(device)\n",
    "            llp = llp.to(device)\n",
    "            llm = llm.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ps = model(texts, masks)\n",
    "            loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                   criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += len(texts)\n",
    "            last_val += len(texts)\n",
    "            summary_writer.add_scalar('Train/loss', loss.item(), step)\n",
    "            if last_val >= 10 * BATCH_SIZE:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    texts, masks, lfp, lfm, llp, llm = next(val_data_loader)\n",
    "                    texts = texts.to(device)\n",
    "                    masks = masks.to(device)\n",
    "                    lfp = lfp.to(device)\n",
    "                    lfm = lfm.to(device)\n",
    "                    llp = llp.to(device)\n",
    "                    llm = llm.to(device)\n",
    "\n",
    "                    ps = model(texts, masks)\n",
    "                    loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                           criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "                    summary_writer.add_scalar('Validation/loss', loss.item(), step)\n",
    "                    \n",
    "                model.train()\n",
    "                last_val = 0\n",
    "                \n",
    "        if scheduler:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pi4TSt51TJx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ec64f0b9b4426aad636d4eb9fffa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0051, 0.0039, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0053, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 0.0000, 5.4260, 5.2162, 5.3045, 0.0000, 5.4664, 0.0000, 5.4367,\n",
      "        5.1557, 5.4544, 5.5503, 5.3841, 5.2862, 0.0000, 5.1992],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0051, 0.0039, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0053, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 0.0000, 5.3758, 5.2586, 5.2254, 0.0000, 5.4499, 0.0000, 5.4214,\n",
      "        5.1732, 5.4104, 5.5633, 5.3624, 5.3687, 0.0000, 5.1895],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.3088, 5.5538, 5.4699, 5.3300, 5.1368, 5.1236, 5.5557, 5.2709, 5.3743,\n",
      "        4.9727, 5.5582, 5.3824, 5.4687, 5.3346, 5.3053, 4.9854],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.3681, 5.5980, 5.4925, 5.3399, 5.0935, 5.1971, 5.5270, 5.2670, 5.4180,\n",
      "        4.9251, 5.5291, 5.3287, 5.4827, 5.3346, 5.3534, 4.9779],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.0000,\n",
      "        0.0000, 0.0041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.6060, 5.5916, 5.5494, 5.3654, 0.0000, 0.0000, 0.0000, 5.5456, 5.1190,\n",
      "        5.3915, 0.0000, 5.3216, 5.1958, 5.5350, 5.3449, 5.5522],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.0000,\n",
      "        0.0000, 0.0041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.5185, 5.6018, 5.5696, 5.2989, 0.0000, 0.0000, 0.0000, 5.5642, 5.1422,\n",
      "        5.4382, 0.0000, 5.3547, 5.1614, 5.5574, 5.3846, 5.5862],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.4486, 5.6020, 5.5697, 5.4635, 5.1807, 5.3828, 5.5010, 5.4136,\n",
      "        5.4578, 5.5416, 5.5096, 0.0000, 5.2036, 5.3389, 5.3926],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.5244, 5.5472, 5.4876, 5.4099, 5.2601, 5.3614, 5.5284, 5.4320,\n",
      "        5.5134, 5.5723, 5.6263, 0.0000, 5.1929, 5.2532, 5.4917],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.0458, 5.0840, 5.4470, 5.3412, 5.5575, 5.2945, 5.3033, 5.5437, 5.0757,\n",
      "        5.0599, 5.0988, 5.3758, 5.5574, 5.4270, 5.2044, 5.2069],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.0415, 5.1384, 5.4207, 5.3377, 5.5049, 5.3863, 5.2931, 5.5583, 5.1052,\n",
      "        5.0624, 5.1209, 5.4035, 5.5676, 5.4118, 5.2273, 5.1456],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0054, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.5315, 5.4813, 0.0000, 5.0292, 5.4957, 5.4891, 0.0000, 5.4632,\n",
      "        5.3026, 5.5933, 5.4506, 5.3563, 5.5700, 5.5535, 5.5365],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0054, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.5479, 5.6061, 0.0000, 5.1276, 5.4718, 5.5330, 0.0000, 5.5921,\n",
      "        5.3202, 5.5009, 5.5316, 5.3458, 5.6101, 5.5015, 5.5515],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0048,\n",
      "        0.0000, 0.0046, 0.0000, 0.0000, 0.0000, 0.0052, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.2378, 5.0958, 5.5055, 5.5264, 0.0000, 5.2638, 5.4538, 5.5056, 0.0000,\n",
      "        5.4808, 0.0000, 5.4334, 5.5667, 5.1245, 0.0000, 5.5104],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0048,\n",
      "        0.0000, 0.0046, 0.0000, 0.0000, 0.0000, 0.0052, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.3130, 5.1078, 5.4550, 5.5308, 0.0000, 5.2487, 5.4366, 5.5703, 0.0000,\n",
      "        5.4519, 0.0000, 5.4136, 5.5376, 5.1233, 0.0000, 5.5843],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0043, 0.0000, 0.0000, 0.0000, 0.0040, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0053],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.2981, 5.5435, 0.0000, 5.5091, 5.5372, 5.5537, 0.0000, 5.5582, 5.4890,\n",
      "        5.5467, 5.3407, 5.2497, 5.5064, 5.3758, 5.5790, 0.0000],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0043, 0.0000, 0.0000, 0.0000, 0.0040, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0053],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.2734, 5.5916, 0.0000, 5.6376, 5.5170, 5.5741, 0.0000, 5.5102, 5.5473,\n",
      "        5.5468, 5.3054, 5.2486, 5.5048, 5.3676, 5.5178, 0.0000],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.0960, 5.4613, 5.2080, 5.5505, 5.1860, 5.5189, 5.4273, 5.5128,\n",
      "        5.2010, 5.0061, 5.4943, 5.5220, 5.5389, 5.2710, 5.3739],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 5.0201, 5.4081, 5.1886, 5.4297, 5.2032, 5.4377, 5.4048, 5.5307,\n",
      "        5.2428, 5.0229, 5.5781, 5.5388, 5.5174, 5.2574, 5.5104],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0040, 0.0000, 0.0000, 0.0000, 0.0042, 0.0000, 0.0039, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.4240, 0.0000, 5.5466, 5.5749, 5.1167, 0.0000, 5.5043, 0.0000, 5.3080,\n",
      "        5.5041, 5.5260, 5.2191, 5.0915, 5.5212, 5.2771, 5.2018],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "tensor([0.0000, 0.0040, 0.0000, 0.0000, 0.0000, 0.0042, 0.0000, 0.0039, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([5.3294, 0.0000, 5.4631, 5.4955, 5.1222, 0.0000, 5.5483, 0.0000, 5.2789,\n",
      "        5.5463, 5.5445, 5.1866, 5.0769, 5.5563, 5.2877, 5.1201],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-9d8ce977423c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-978680dcbdbb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TextClassifier()\n",
    "model.to(device)\n",
    "\n",
    "criterion = MaskedLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_1,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
    "\n",
    "train(model, criterion, optimizer, scheduler, EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((2, 4)).sum()\n",
    "print(a.shape)\n",
    "b = torch.max(a, 1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odHkHiwg3qy_"
   },
   "outputs": [],
   "source": [
    "#train(model, criterion, optimizer, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr62wfbn1TJ4"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_2,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
    "\n",
    "train(model, criterion, optimizer, scheduler, EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(\n",
    "#    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#    0.00001,\n",
    "#    weight_decay=W_L2_NORM\n",
    "#)\n",
    "\n",
    "#train(model, criterion, optimizer, scheduler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ic07H3yj42z3"
   },
   "outputs": [],
   "source": [
    "def get_best(ps):\n",
    "    n = len(ps)\n",
    "    first, last, mx = 0, 0, 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            lmx = ps[i, 0] * ps[j, 1]\n",
    "            if mx < lmx:\n",
    "                mx, first,last = lmx, i, j\n",
    "                \n",
    "    return first, last, mx\n",
    "\n",
    "with open(PATH_DATASET_TEST, 'r') as file:\n",
    "    dataset_test = file.readlines()[1:]\n",
    "    \n",
    "res = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(dataset_test):\n",
    "        _, question_id, paragraph, question = sample.split('\\t')\n",
    "\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        text_tokens = tokenizer.tokenize(paragraph)\n",
    "        \n",
    "        all_tokens = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     text_tokens + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "        length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "        if (len(text_tokens) > length):\n",
    "            part_length = length // 3\n",
    "            stride = 3 * part_length\n",
    "            nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "            indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "            indexes = indexes.astype(np.int32)\n",
    "\n",
    "            max_index = indexes.max()\n",
    "            diff = max_index + 1 - len(text_tokens)\n",
    "            text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "            text_tokens = np.array(text_tokens)[indexes].tolist()\n",
    "\n",
    "            first, last, mx = 0, 0, 0\n",
    "            for i, ts in enumerate(text_tokens):\n",
    "                while ts[-1] == tokenizer.pad_token:\n",
    "                    ts = ts[:-1]\n",
    "\n",
    "                ts = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     ts + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "                texts, masks = text_collate_fn([ts])\n",
    "                texts = texts.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                lps = model(texts, masks)[0]\n",
    "                lps = lps[2 + len(question_tokens):]\n",
    "                lps[:, 0] = F.softmax(lps[:, 0], 0)\n",
    "                lps[:, 1] = F.softmax(lps[:, 1], 0)\n",
    "                lps = lps.cpu().numpy()\n",
    "                \n",
    "                lfirst, llast, lmx = get_best(lps)\n",
    "                if mx < lmx:\n",
    "                    mx = lmx\n",
    "                    first = lfirst + i * part_length\n",
    "                    last = llast + i * part_length\n",
    "                \n",
    "            first += 2 + len(question_tokens)\n",
    "            last += 2 + len(question_tokens)\n",
    "                    \n",
    "        else:\n",
    "            texts, masks = text_collate_fn([all_tokens])\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            ps = model(texts, masks)[0]\n",
    "            ps[:, 0] = F.softmax(ps[:, 0], 0)\n",
    "            ps[:, 1] = F.softmax(ps[:, 1], 0)\n",
    "            ps = ps.cpu().numpy()\n",
    "            \n",
    "            first, last, _ = get_best(ps)\n",
    "            \n",
    "        s = ''\n",
    "        tokens = all_tokens[first:last + 1]\n",
    "        for token in tokens:\n",
    "            if token == tokenizer.unk_token:\n",
    "                continue\n",
    "            \n",
    "            if token[0] == '#':\n",
    "                s += token.replace('#', '')\n",
    "            else:\n",
    "                s += ' ' + token\n",
    "            \n",
    "        res += [question_id + '\\t' + s.strip()]\n",
    "        \n",
    "res = '\\n'.join(res)\n",
    "with open(PATH_RESULTS, 'w') as file:\n",
    "    file.write(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
