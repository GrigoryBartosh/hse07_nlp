{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "PATH_DATASET = os.path.join('data', 'train_qa.csv')\n",
    "\n",
    "MAX_TEXT_LEN = 512\n",
    "\n",
    "EPOCHS_1 = 40\n",
    "EPOCHS_2 = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE_1 = 0.0001\n",
    "LEARNING_RATE_2 = 0.0001\n",
    "W_L2_NORM = 0.0\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(PATH_DATASET)\n",
    "dataset_texts = dataset['paragraph']\n",
    "dataset_questions = dataset['question']\n",
    "dataset_answers = dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(text, question, answer):\n",
    "    answer = answer.lower()\n",
    "    while (answer[0] == '.'):\n",
    "        answer = answer[1:]\n",
    "    while (answer[-1] in ['.', '?']):\n",
    "        answer = answer[:-1]\n",
    "        \n",
    "    if answer not in text.lower():\n",
    "        return [], []\n",
    "    \n",
    "    first = text.lower().find(answer)\n",
    "    last = first + len(answer)\n",
    "    \n",
    "    text_1 = text[:first].strip()\n",
    "    text_2 = text[first:last].strip()\n",
    "    text_3 = text[last:].strip()\n",
    "    text_tokens = tokenizer.tokenize(text_1)\n",
    "    first = len(text_tokens)\n",
    "    text_tokens += tokenizer.tokenize(text_2)\n",
    "    last = len(text_tokens) - 1\n",
    "    text_tokens += tokenizer.tokenize(text_3)\n",
    "    \n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    length = np.random.randint(\n",
    "        min(MAX_TEXT_LEN - len(question_tokens) - 3, 200), \n",
    "        MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "    )\n",
    "    if len(text_tokens) > length:\n",
    "        part_length = length // 3\n",
    "        stride = 3 * part_length\n",
    "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "        indexes = indexes.astype(np.int32)\n",
    "\n",
    "        max_index = indexes.max()\n",
    "        diff = max_index + 1 - len(text_tokens)\n",
    "        text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "        text_tokens = np.array(text_tokens)[indexes].tolist()\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for i, ts in enumerate(text_tokens):\n",
    "            while ts[-1] == tokenizer.pad_token:\n",
    "                ts = ts[:-1]\n",
    "                \n",
    "            tokens += [ts]\n",
    "                \n",
    "            lfirst = first - i * part_length\n",
    "            llast = last - i * part_length\n",
    "            \n",
    "            labels += [((lfirst if lfirst >= 0 and lfirst < len(ts) else 0,\n",
    "                         (lfirst >= part_length and lfirst < 2 * part_length) or \n",
    "                         (i == 0 and lfirst < part_length) or\n",
    "                         (i == len(text_tokens) - 1 and lfirst >= 2 * part_length)),\n",
    "                        (llast if llast >= 0 and llast < len(ts) else 0,\n",
    "                         (llast >= part_length and llast < 2 * part_length) or \n",
    "                         (i == 0 and llast < part_length) or\n",
    "                         (i == len(text_tokens) - 1 and llast >= 2 * part_length)))]\n",
    "    else:\n",
    "        tokens = [text_tokens]\n",
    "        labels = [((first, 1), (last, 1))]\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = [tokenizer.cls_token] + \\\n",
    "                    tokens[i] + \\\n",
    "                    [tokenizer.sep_token] + \\\n",
    "                    question_tokens + \\\n",
    "                    [tokenizer.sep_token]\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokens, dataset_labels = [], []\n",
    "\n",
    "for text, question, answer in tqdm(zip(dataset_texts, dataset_questions, dataset_answers)):\n",
    "    tokens, labels = prepare_sample(text, question, answer)\n",
    "    dataset_tokens += tokens\n",
    "    dataset_labels += labels \n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset_tokens, dataset_labels, test_size=0.1)\n",
    "train_data = list(zip(x_train, y_train))\n",
    "val_data = list(zip(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_collate_fn(texts):\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
    "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
    "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
    "    texts = torch.LongTensor(texts)\n",
    "    masks = torch.LongTensor(masks)\n",
    "\n",
    "    return texts, masks\n",
    "\n",
    "def collate_fn(data):\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    texts, masks = text_collate_fn(texts)\n",
    "    \n",
    "    labels_first, labels_last = zip(*labels)\n",
    "    labels_first_pos, labels_first_valid = zip(*labels_first)\n",
    "    labels_last_pos, labels_last_valid = zip(*labels_last)\n",
    "    \n",
    "    labels_first_pos = torch.LongTensor(labels_first_pos)\n",
    "    labels_first_mask = torch.LongTensor(labels_first_valid)\n",
    "    labels_last_pos = torch.LongTensor(labels_last_pos)\n",
    "    labels_last_mask = torch.LongTensor(labels_last_valid)\n",
    "    \n",
    "    return texts, masks, labels_first_pos, labels_first_mask, labels_last_pos, labels_last_mask\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(16, 2),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        x = self.bert(text, attention_mask=mask)[0]\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    EPS  = 1e-8\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "\n",
    "    def masked_softmax(self, vec, mask, dim=1):\n",
    "        masked_vec = vec * mask.float()\n",
    "        max_vec = torch.max(masked_vec, dim=dim, keepdim=True)[0]\n",
    "        exps = torch.exp(masked_vec - max_vec)\n",
    "        masked_exps = exps * mask.float()\n",
    "        masked_sums = masked_exps.sum(dim, keepdim=True)\n",
    "        zeros = (masked_sums == 0)\n",
    "        masked_sums += zeros.float()\n",
    "        return masked_exps / masked_sums\n",
    "        \n",
    "    def forward(self, output, output_mask, target, target_mask):\n",
    "        log_0 = -torch.log(1 - output + MaskedLoss.EPS)\n",
    "        log_0_mask = output_mask * (1 - target_mask)[:, None]\n",
    "        log_0 = log_0 * log_0_mask\n",
    "        \n",
    "        sm = self.masked_softmax(output, output_mask)\n",
    "        sm = torch.gather(output, 1, target[:, None])\n",
    "        sm = sm * target_mask\n",
    "        \n",
    "        loss = log_0.sum() / log_0_mask.sum() + sm.sum() / target_mask.sum()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epochs):\n",
    "    losses_train, losses_val = [], []\n",
    "    for _ in trange(epochs):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for texts, masks, lfp, lfm, llp, llm in train_data_loader:\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "            lfp = lfp.to(device)\n",
    "            lfm = lfm.to(device)\n",
    "            llp = llp.to(device)\n",
    "            llm = llm.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ps = model(texts, masks)\n",
    "            loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                   criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        losses_train.append(np.array(losses).mean())\n",
    "\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for texts, masks, labels in val_data_loader:\n",
    "                texts = texts.to(device)\n",
    "                masks = masks.to(device)\n",
    "                lfp = lfp.to(device)\n",
    "                lfm = lfm.to(device)\n",
    "                llp = llp.to(device)\n",
    "                llm = llm.to(device)\n",
    "                \n",
    "                ps = model(texts, masks)\n",
    "                loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
    "                       criterion(ps[:, :, 1], masks, llp, llm)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        losses_val.append(np.array(losses).mean())\n",
    "\n",
    "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier()\n",
    "model.to(device)\n",
    "\n",
    "criterion = MaskedLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_1,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "\n",
    "train(model, criterion, optimizer, EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([32, 369])\n",
    "b = torch.zeros([32])[:, None]\n",
    "c = a * b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    LEARNING_RATE_2,\n",
    "    weight_decay=W_L2_NORM\n",
    ")\n",
    "\n",
    "train(model, criterion, optimizer, EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(text):\n",
    "    parts, parts_poses = [], []\n",
    "    part, part_start = '', 0\n",
    "    for i, c in enumerate(text):\n",
    "        if c.isalpha() or (c.isdigit() and part != '' and part[-1].isdigit()):\n",
    "            part += c\n",
    "        else:\n",
    "            if len(part) > 0:\n",
    "                parts.append(part)\n",
    "                parts_poses.append((part_start, i))\n",
    "\n",
    "            if c.isdigit():\n",
    "                part, part_start = c, i\n",
    "            else:\n",
    "                if c != ' ':\n",
    "                    parts.append(c)\n",
    "                    parts_poses.append((i, i + 1))\n",
    "\n",
    "                part, part_start = '', i + 1\n",
    "\n",
    "    parts.append(part)\n",
    "    parts_poses.append((part_start, len(text)))\n",
    "\n",
    "    tokens, tokens_poses = [], []\n",
    "    for part, poses in zip(parts, parts_poses):\n",
    "        new_tokens = tokenizer.tokenize(part)\n",
    "        tokens_poses += [poses for _ in new_tokens]\n",
    "        tokens += new_tokens\n",
    "\n",
    "    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "    tokens_poses = [(-1, -1)] + tokens_poses + [(-1, -1)]\n",
    "\n",
    "    return tokens, tokens_poses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
