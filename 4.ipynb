{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/hse07_nlp/blob/master/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-12U8rg1TJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVy7cjtt1TJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.CRITICAL)\n",
        "\n",
        "PATH_DATASET = os.path.join('data', 'train_qa.csv')\n",
        "\n",
        "PATH_DATASET_TEST = os.path.join('data', 'test.txt')\n",
        "PATH_RESULTS = os.path.join('data', 'results.txt')\n",
        "\n",
        "MAX_TEXT_LEN = 512\n",
        "\n",
        "EPOCHS_1 = 40\n",
        "EPOCHS_2 = 5\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE_1 = 0.0001\n",
        "LEARNING_RATE_2 = 0.0001\n",
        "W_L2_NORM = 0.0\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOec3Kj71TJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(PATH_DATASET)\n",
        "dataset_texts = dataset['paragraph']\n",
        "dataset_questions = dataset['question']\n",
        "dataset_answers = dataset['answer']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHk4_axm1TJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    do_lower_case=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9319rsQ1TJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sample(text, question, answer):\n",
        "    answer = answer.lower()\n",
        "    while (answer[0] == '.'):\n",
        "        answer = answer[1:]\n",
        "    while (answer[-1] in ['.', '?']):\n",
        "        answer = answer[:-1]\n",
        "        \n",
        "    if answer not in text.lower():\n",
        "        return [], []\n",
        "    \n",
        "    first = text.lower().find(answer)\n",
        "    last = first + len(answer)\n",
        "    \n",
        "    text_1 = text[:first].strip()\n",
        "    text_2 = text[first:last].strip()\n",
        "    text_3 = text[last:].strip()\n",
        "    text_tokens = tokenizer.tokenize(text_1)\n",
        "    first = len(text_tokens)\n",
        "    text_tokens += tokenizer.tokenize(text_2)\n",
        "    last = len(text_tokens) - 1\n",
        "    text_tokens += tokenizer.tokenize(text_3)\n",
        "    \n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    \n",
        "    length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
        "    if len(text_tokens) > length:\n",
        "        part_length = length // 3\n",
        "        stride = 3 * part_length\n",
        "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
        "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
        "        indexes = indexes.astype(np.int32)\n",
        "\n",
        "        max_index = indexes.max()\n",
        "        diff = max_index + 1 - len(text_tokens)\n",
        "        text_tokens += diff * [tokenizer.pad_token]\n",
        "\n",
        "        text_tokens = np.array(text_tokens)[indexes].tolist()\n",
        "        \n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for i, ts in enumerate(text_tokens):\n",
        "            while ts[-1] == tokenizer.pad_token:\n",
        "                ts = ts[:-1]\n",
        "                \n",
        "            tokens += [ts]\n",
        "                \n",
        "            lfirst = first - i * part_length\n",
        "            llast = last - i * part_length\n",
        "            \n",
        "            labels += [(((lfirst if lfirst >= 0 and lfirst < len(ts) else 0) + 2 + len(question_tokens),\n",
        "                         lfirst >= 0 and lfirst < len(ts)),\n",
        "                        ((llast if llast >= 0 and llast < len(ts) else 0) + 2 + len(question_tokens),\n",
        "                         llast >= 0 and llast < len(ts)))]\n",
        "    else:\n",
        "        tokens = [text_tokens]\n",
        "        labels = [((first, 1), (last, 1))]\n",
        "        \n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = [tokenizer.cls_token] + \\\n",
        "                    question_tokens + \\\n",
        "                    [tokenizer.sep_token] + \\\n",
        "                    tokens[i] + \\\n",
        "                    [tokenizer.sep_token]\n",
        "\n",
        "    return tokens, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-47IcN861TJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_tokens, dataset_labels = [], []\n",
        "\n",
        "for text, question, answer in tqdm(zip(dataset_texts, dataset_questions, dataset_answers)):\n",
        "    tokens, labels = prepare_sample(text, question, answer)\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels \n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(dataset_tokens, dataset_labels, test_size=0.1)\n",
        "train_data = list(zip(x_train, y_train))\n",
        "val_data = list(zip(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPcZDQkT1TJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_collate_fn(texts):\n",
        "    max_len = max([len(text) for text in texts])\n",
        "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
        "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
        "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
        "    texts = torch.LongTensor(texts)\n",
        "    masks = torch.LongTensor(masks)\n",
        "\n",
        "    return texts, masks\n",
        "\n",
        "def collate_fn(data):\n",
        "    texts, labels = zip(*data)\n",
        "\n",
        "    texts, masks = text_collate_fn(texts)\n",
        "    \n",
        "    labels_first, labels_last = zip(*labels)\n",
        "    labels_first_pos, labels_first_valid = zip(*labels_first)\n",
        "    labels_last_pos, labels_last_valid = zip(*labels_last)\n",
        "    \n",
        "    labels_first_pos = torch.LongTensor(labels_first_pos)\n",
        "    labels_first_mask = torch.LongTensor(labels_first_valid)\n",
        "    labels_last_pos = torch.LongTensor(labels_last_pos)\n",
        "    labels_last_mask = torch.LongTensor(labels_last_valid)\n",
        "    \n",
        "    return texts, masks, labels_first_pos, labels_first_mask, labels_last_pos, labels_last_mask\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_data_loader = data.DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IFl_5gL1TJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(16, 2)\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, text, mask):\n",
        "        x = self.bert(text, attention_mask=mask)[0]\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6H8Vd4R1TJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedLoss(nn.Module):\n",
        "    EPS  = 1e-8\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MaskedLoss, self).__init__()\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def masked_softmax(self, vec, mask, dim=1):\n",
        "        masked_vec = vec * mask.float()\n",
        "        max_vec = torch.max(masked_vec, dim=dim, keepdim=True)[0]\n",
        "        exps = torch.exp(masked_vec - max_vec)\n",
        "        masked_exps = exps * mask.float()\n",
        "        masked_sums = masked_exps.sum(dim, keepdim=True)\n",
        "        zeros = (masked_sums == 0)\n",
        "        masked_sums += zeros.float()\n",
        "        return masked_exps / masked_sums\n",
        "        \n",
        "    def forward(self, output, output_mask, target, target_mask):\n",
        "        output_s = self.sigmoid(output)\n",
        "\n",
        "        log_0 = -torch.log(1 - output_s + MaskedLoss.EPS)\n",
        "        log_0 = log_0 * output_mask\n",
        "\n",
        "        output_t = torch.gather(output_s, 1, target[:, None]).squeeze()\n",
        "        log_0_t = -torch.log(1 - output_t + MaskedLoss.EPS)\n",
        "        log_1_t = -torch.log(output_t + MaskedLoss.EPS)\n",
        "        log_0_t = log_0_t * target_mask\n",
        "        log_1_t = log_1_t * target_mask\n",
        "        \n",
        "        sm = self.masked_softmax(output, output_mask)\n",
        "        sm = torch.gather(sm, 1, target[:, None]).squeeze()\n",
        "        sm = sm * target_mask\n",
        "        \n",
        "        loss = (log_0.sum() - log_0_t.sum()) / (output_mask.sum() - target_mask.sum()) + \\\n",
        "               log_1_t.sum() / target_mask.sum() + \\\n",
        "               sm.sum() / target_mask.sum()\n",
        "        \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx8Mkluo1TJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, epochs):\n",
        "    losses_train, losses_val = [], []\n",
        "    for _ in trange(epochs):\n",
        "        losses = []\n",
        "        model.train()\n",
        "        for texts, masks, lfp, lfm, llp, llm in train_data_loader:\n",
        "            texts = texts.to(device)\n",
        "            masks = masks.to(device)\n",
        "            lfp = lfp.to(device)\n",
        "            lfm = lfm.to(device)\n",
        "            llp = llp.to(device)\n",
        "            llm = llm.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ps = model(texts, masks)\n",
        "            loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
        "                   criterion(ps[:, :, 1], masks, llp, llm)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        losses_train.append(np.array(losses).mean())\n",
        "\n",
        "        losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for texts, masks, lfp, lfm, llp, llm in val_data_loader:\n",
        "                texts = texts.to(device)\n",
        "                masks = masks.to(device)\n",
        "                lfp = lfp.to(device)\n",
        "                lfm = lfm.to(device)\n",
        "                llp = llp.to(device)\n",
        "                llm = llm.to(device)\n",
        "                \n",
        "                ps = model(texts, masks)\n",
        "                loss = criterion(ps[:, :, 0], masks, lfp, lfm) + \\\n",
        "                       criterion(ps[:, :, 1], masks, llp, llm)\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        losses_val.append(np.array(losses).mean())\n",
        "\n",
        "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
        "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pi4TSt51TJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TextClassifier()\n",
        "model.to(device)\n",
        "\n",
        "criterion = MaskedLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_1,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "#train(model, criterion, optimizer, EPOCHS_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odHkHiwg3qy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, criterion, optimizer, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr62wfbn1TJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    LEARNING_RATE_2,\n",
        "    weight_decay=W_L2_NORM\n",
        ")\n",
        "\n",
        "train(model, criterion, optimizer, EPOCHS_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic07H3yj42z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(PATH_DATASET_TEST, 'r') as file:\n",
        "    dataset_test = file.readlines()[1:]\n",
        "    \n",
        "res[]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sample in dataset_test:\n",
        "        _, question_id, paragraph, question = sample.split('\\t')\n",
        "\n",
        "        tokens = [tokenizer.cls_token] + \\\n",
        "                tokenizer.tokenize(question) + \\\n",
        "                [tokenizer.sep_token] + \\\n",
        "                tokenizer.tokenize(paragraph) + \\\n",
        "                [tokenizer.sep_token]\n",
        "\n",
        "        if (len(tokens) > MAX_TEXT_LEN):\n",
        "            res += [question_id + u'я ваще хз']\n",
        "            continue\n",
        "\n",
        "        texts, masks = text_collate_fn([tokens])\n",
        "        texts = texts.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        ps = model(texts, masks)[0]\n",
        "        ps = ps.cpu().numpy()\n",
        "        first = ps[:, 0].argmax()\n",
        "        last = ps[:, 1].argmax()\n",
        "\n",
        "        res += [question_id + ' '.join(tokens[first, last])]\n",
        "\n",
        "res = '\\n'.join(res)\n",
        "with open(PATH_RESULTS, 'w') as file:\n",
        "    file.write(res)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}